{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Дилемма смещения и разброса. Полиномиальные признаки. Регуляризация**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "# всё остальное\n",
    "from sklearn import linear_model\n",
    "import numpy as np #для матричных вычислений\n",
    "import pandas as pd #для анализа и предобработки данных\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt #для визуализации\n",
    "import seaborn as sns #для визуализации\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn') #установка стиля matplotlib\n",
    "\n",
    "from sklearn.datasets import load_boston "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **СМЕЩЕНИЕ И РАЗБРОС**\n",
    "\n",
    "Центральной проблемой всего обучения с учителем (не только линейных моделей) является ***дилемма смещения и разброса модели***. Давайте узнаем, что это такое.\n",
    "\n",
    "До этого момента мы обучали модели на всех имеющихся данных. С одной стороны, это имеет смысл, ведь мы хотим минимизировать ошибки модели, используя как можно больше данных для обучения.\n",
    "\n",
    "С другой стороны, из-за такого подхода становится труднее оценивать, насколько хорошо работает модель. Причина этого в том, что, если мы продолжим рассчитывать метрики, используя тренировочные данные, мы можем обнаружить, что при применении модели на незнакомых ей данных она работает довольно плохо.\n",
    "\n",
    "*Таким образом, модель может детально подстроиться под зависимость в обучающей выборке, но не уловить общей сути.*\n",
    "\n",
    "Такая проблема называется **переобучением** (overfitting). По сути, такая модель работает намного лучше с обучающими данными, чем с новыми. Она была чрезмерно натренирована на обнаружение уникальных характеристик обучающего набора данных, которые не являются общими закономерностями.\n",
    "\n",
    "**Недообучение** (underfitting) — проблема, обратная переобучению. Модель из-за своей слабости не уловила никаких закономерностей в данных. В этом случае ошибка будет высокой как для тренировочных данных, так и для данных, не показанных во время обучения.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/24d0e131092f71d843db3a5fe85cdbf5/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml2-5_1.png)\n",
    "\n",
    "* На первом рисунке изображена простая модель линейной регрессии, не способная уловить сложную зависимость в данных.\n",
    "* На втором рисунке изображена оптимальная модель, которая хорошо описывает зависимость и при этом не имеет переобучения (полином четвёртой степени).\n",
    "* На последнем рисунке изображен полином 27-й степени, который подстроился под каждую точку в тренировочном наборе, но не смог уловить общие закономерности.\n",
    "***\n",
    "С теоретической точки зрения недообучение и переобучение характеризуются понятиями **смещения** и **разброса** модели.\n",
    "***\n",
    "**Смещение (bias)** — это математическое ожидание разности между истинным ответом и ответом, выданным моделью. То есть это **ожидаемая ошибка модели**.\n",
    "\n",
    "![](data\\f49.png)\n",
    "\n",
    "В зарубежной литературе математическое ожидание часто обозначается как ***E***:\n",
    "\n",
    "![](data\\f50.png)\n",
    "\n",
    "Чем больше смещение, тем слабее модель. Если модель слабая, она не в состоянии выучить закономерность. Таким образом, налицо недообучение модели.\n",
    "***\n",
    "**Разброс (variance)** — это вариативность ошибки, то, насколько ошибка будет отличаться, если обучать модель на разных наборах данных. Математически это дисперсия (разброс) ответов модели.\n",
    "\n",
    "![](data\\f51.png)\n",
    "\n",
    "Примечание. В зарубежной литературе дисперсия часто обозначается как ***Var***:\n",
    "\n",
    "![](data\\f52.png)\n",
    "\n",
    "Чем больше разброс, тем больше ошибка будет колебаться на разных наборах данных. Наличие высокого разброса и есть свидетельство переобучения: модель подстроилась под конкретный набор данных и даёт высокий разброс ответов на разных данных.\n",
    "***\n",
    "Теоретически на составляющие смещения и разброса модели можно разложить любую функцию потерь. Например, разложение квадратичной ошибки (её математическое ожидание) будет выглядеть следующим образом:\n",
    "\n",
    "![](data\\f53.png)\n",
    "\n",
    "Математическое ожидание — это среднее значение во всей генеральной совокупности (на бесконечной выборке), а не на конкретных значениях. Это исключительно теоретическая величина.\n",
    "\n",
    "* ***σ*** — неустранимая ошибка, вызванная случайностью.\n",
    "* ***bias(y)^2*** — смещение модели (в квадрате).\n",
    "* ***variance(y)*** — разброс модели.\n",
    "\n",
    "**О чём нам говорит эта теоретическая формула?**\n",
    "\n",
    "Ошибка модели складывается из смещения модели (в квадрате) и её разброса, а также случайной ошибки. \n",
    "\n",
    "Если с последним слагаемым  мы ничего не сможем сделать, то вот на первые два (bias и variance) мы можем как-то повлиять. В идеале мы должны свести их к 0. Однако уменьшение одного слагаемого повлечёт увеличение другого. На практике часто приходится балансировать между смещёнными и нестабильными оценками.\n",
    "Дилемма смещения-дисперсии является центральной проблемой в обучении с учителем. В идеале мы хотим построить модель, которая точно описывает зависимости в тренировочных данных и хорошо работает на неизвестных данных. К сожалению, обычно это невозможно сделать одновременно.\n",
    "\n",
    "* **Усложняя** модель, мы пытаемся **уменьшить смещение (bias)**, однако появляется риск получить переобучение, то есть мы **повышаем разброс (variance)**. \n",
    "* С другой стороны, **снизить разброс (variance)** позволяют более **простые модели**, не склонные к переобучению, но есть риск, что простая модель не уловит зависимостей и окажется недообученной, то есть мы **повышаем смещение (bias)**.\n",
    "\n",
    "Если провести аналогию, что модель — это игрок в дартс, то самый лучший игрок будет иметь небольшое смещение и разброс: его дротики ложатся кучно «в яблочко». Если у игрока большое смещение, то дротики сгруппированы около другой точки, а если большой разброс — дротики разлетаются по всей мишени.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/c9e31e930c613a02ef6abd7c363158d6/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml2-5_2.png)\n",
    "***\n",
    "Теперь, когда мы знаем о теоретических основах проблемы переобучения и недообучения, что мы можем сделать, чтобы лучше судить о способности модели к обобщению на практике? Как диагностировать высокие bias и variance?\n",
    "\n",
    "Типичным решением является разделение данных на две части: **обучающий** и **тестовый** наборы. На тренировочном наборе данных мы будем обучать модель, подбирая параметры. Тестовый набор данных, который модель не видела при обучении, мы будем использовать для оценки истинного качества моделирования. Схематично можно представить это следующим образом:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/6b3371e3f1d290f0c216ac1e826005ff/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml2-5_3.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим, как это работает на практике. Работать будем с уже знакомыми нам данными — данными о домах в Бостоне."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\local_admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston \n",
    "\n",
    "boston = load_boston()\n",
    "#создаём DataFrame из загруженных numpy-матриц\n",
    "boston_data = pd.DataFrame(\n",
    "    data=boston.data, #данные\n",
    "    columns=boston.feature_names #наименования столбцов\n",
    ")\n",
    "#добавляем в таблицу столбец с целевой переменной\n",
    "boston_data['MEDV'] = boston.target\n",
    " \n",
    "#Составляем список факторов (исключили целевой столбец)\n",
    "features = boston_data.drop('MEDV', axis=1).columns\n",
    "#Составляем матрицу наблюдений X и вектор ответов y\n",
    "X = boston_data[features]\n",
    "y = boston_data['MEDV']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В **sklearn** для разделения выборки на тренировочную и тестовую есть функция [**train_test_split()**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) из модуля **model_selection**. Данная функция принимает следующие аргументы:\n",
    "\n",
    "* ***X*** и ***y*** — таблица с примерами и ответами к ним.\n",
    "* ***random_state*** — число, на основе которого генерируются случайные числа. Тренировочная и тестовая выборка генерируются случайно. Чтобы эксперимент был воспроизводимым, необходимо установить этот параметр в конкретное значение.\n",
    "* ***test_size*** — доля тестовой выборки. Параметр определяет, в каких пропорциях будет разделена выборка. Стандартные значения: 70/30, 80/20.\n",
    "\n",
    "Функция возвращает четыре объекта в следующем порядке: тренировочные примеры, тестовые примеры, тренировочные ответы и тестовые ответы. \n",
    "\n",
    "Итак, давайте разделим нашу выборку на тренировочную и тестовую в соотношении 70/30:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (354, 13) (354,)\n",
      "Test: (152, 13) (152,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#Разделяем выборку на тренировочную и тестовую в соотношении 70/30\n",
    "#Устанавливаем random_state для воспроизводимости результатов \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=40)\n",
    "#Выводим результирующие размеры таблиц\n",
    "print('Train:', X_train.shape, y_train.shape)\n",
    "print('Test:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После разделения в тренировочной выборке оказались 354 наблюдения, а в тестовой — 152.\n",
    "\n",
    "Затем обучим линейную регрессию (с помощью МНК) на тренировочных данных и рассчитаем R^2 для тренировочных и тестовых данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R^2: 0.743\n",
      "Test R^2: 0.722\n"
     ]
    }
   ],
   "source": [
    "#Создаём объект класса LinearRegression\n",
    "lr_model = linear_model.LinearRegression()\n",
    "#Обучаем модель по МНК\n",
    "lr_model.fit(X_train, y_train)\n",
    " \n",
    "#Делаем предсказание для тренировочной выборки\n",
    "y_train_predict = lr_model.predict(X_train)\n",
    "#Делаем предсказание для тестовой выборки\n",
    "y_test_predict = lr_model.predict(X_test)\n",
    " \n",
    "print(\"Train R^2: {:.3f}\".format(metrics.r2_score(y_train, y_train_predict)))\n",
    "print(\"Test R^2: {:.3f}\".format(metrics.r2_score(y_test, y_test_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, ***R^2 = 0.743*** на тренировочной выборке и ***R^2 = 0.722*** на тестовой выборке. То есть показатели довольно близки друг к другу (низкий разброс ответов модели для разных выборок).\n",
    "\n",
    "Это одно из свидетельств отсутствия переобучения. Это не удивительно, ведь линейная регрессия, построенная на 13 факторах, является довольно простой моделью: всего лишь 14 параметров, что очень мало по меркам машинного обучения. Риск переобучения возрастает с количеством факторов, которые участвуют в обучении модели.\n",
    "\n",
    "Но что насчёт смещения? Самый простой способ оценить смещение и недообученность модели — посмотреть на значение метрики и интуитивно оценить её.\n",
    "\n",
    "Может, наша модель слишком слабая? ***R^2 = 0.722*** — не слишком уж высокий показатель (напомним, максимум — 1). Возможно, стоит попробовать обучить более сложную модель. Например, можно построить модель полиномиальной регрессии.\n",
    "\n",
    "Примечание. Существуют более совершенные визуальные способы оценить наличие смещения и разброса, такие как кривая обучения и кросс-валидация. О них мы ещё поговорим далее в курсе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ПОЛИНОМИАЛЬНЫЕ ПРИЗНАКИ**\n",
    "\n",
    "**Полиномиальная регрессия (Polynomial Regression)** — это более сложная модель, чем линейная регрессия. Вместо уравнения прямой используется уравнение полинома (многочлена). Степень полинома может быть сколь угодно большой: чем больше степень, тем сложнее модель.\n",
    "\n",
    "В простом двумерном случае, когда мы рассматриваем зависимость целевого признака от одного фактора, полиномом второй степени будет уравнение параболы:\n",
    "\n",
    "![](data\\f55.png)\n",
    "\n",
    "Геометрически полином в двумерном пространстве — это некоторая кривая, которая пытается описать зависимость в данных. Выглядит это следующим образом:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/01a58ba6150976bb3c408323a9b5fbcf/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml2-5_5.png)\n",
    "\n",
    "Когда факторов больше одного, например два, то, помимо возведения фактора в квадрат, появляются ещё и комбинации из ***x1*** и ***x2***:\n",
    "\n",
    "![](data\\f56.png)\n",
    "\n",
    "Такая модель будет описывать сложную поверхность в трёхмерном пространстве, которая проставлена на рисунке:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/10616697357bc2252dd39b8fa419ffc0/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml2-5_6.png)\n",
    "\n",
    "Заметьте, как быстро растёт количество коэффициентов, а с ним и сложность модели. А ведь это только два фактора — ***x1*** и ***x2***. Мы не будем приводить уравнение для общего случая, как делали это с линейной регрессией, так как оно будет содержать слишком много слагаемых.\n",
    "\n",
    "Например, рассматривая построенный ранее график зависимости медианной цены домов от процента низкостатусного населения, вы могли заметить, что между данными показателями существует не линейная, а скорее полиномиальная связь.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/4b02bf0809fb707c3950ce0aa275368e/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml2-5_7.png)\n",
    "\n",
    "Заметим, что степени ***x*** можно тоже считать своего рода искусственными признаками в данных. Они называются полиномиальными признаками.\n",
    "\n",
    "Поэтому полиномиальная регрессия — это та же линейная регрессия, просто с новыми признаками. Полиномиальные признаки — один из самых распространённых методов FeatureEngineering.\n",
    "\n",
    "Пример:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/c9664ed6fac637130e49aa29ebfb67e9/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml2-5_8.png)\n",
    "\n",
    "Благодаря степенным слагаемым модель становится сложнее и начинает улавливать более сложные зависимости и выдавать меньшее смещение. Но, как вы понимаете, резко повышается риск переобучения модели — увеличивается разброс предсказаний на разных данных из-за количества факторов.\n",
    "***\n",
    "Давайте проследим за этим.\n",
    "\n",
    "Построить полиномиальную регрессию в sklearn очень просто. Для начала необходимо создать полиномиальные признаки с помощью объекта класса [**PolynomialFeatures**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) из модуля **preprocessing**. Это преобразователь, который позволит сгенерировать полиномиальные признаки любой степени и добавить их в таблицу. У него есть два важных параметра:\n",
    "\n",
    "* **degree** — степень полинома. По умолчанию используется степень 2.\n",
    "* **include_bias** — включать ли в результирующую таблицу столбец из единиц (x в степени 0). По умолчанию стоит True, но лучше выставить его в значение **False**, так как столбец из единиц и так добавляется в методе наименьших квадратов.\n",
    "\n",
    "*Примечание. Как правило, дата-сайентисты останавливаются на полиноме второй (максимум третьей) степени. Чем выше степень полинома, тем больше слагаемых, а значит, тем больше признаков и тем сложнее становится модель.*\n",
    "\n",
    "Для того чтобы подогнать генератор и рассчитать количество комбинаций степеней, мы используем метод **fit()**, а чтобы сгенерировать новую таблицу признаков, в которую будут включены полиномиальные признаки, используется метод **transform()**, в который нужно передать выборки:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(354, 104)\n",
      "(152, 104)\n"
     ]
    }
   ],
   "source": [
    "# Создаём генератор полиномиальных признаков\n",
    "poly = preprocessing.PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly.fit(X_train)\n",
    "#Генерируем полиномиальные признаки для тренировочной выборки\n",
    "X_train_poly = poly.transform(X_train)\n",
    "#Генерируем полиномиальные признаки для тестовой выборки\n",
    "X_test_poly = poly.transform(X_test)\n",
    "#Выводим результирующие размерности таблиц\n",
    "print(X_train_poly.shape)\n",
    "print(X_test_poly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# В результате мы получили два numpy-массива:\n",
    "print(type(X_train_poly))\n",
    "print(type(X_test_poly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R^2: 0.929\n",
      "Test R^2: 0.268\n"
     ]
    }
   ],
   "source": [
    "# Теперь попробуем скормить наши данные модели линейной регрессии, чтобы найти коэффициенты полинома по МНК-алгоритму:\n",
    "\n",
    "#Создаём объект класса LinearRegression\n",
    "lr_model_poly = linear_model.LinearRegression()\n",
    "#Обучаем модель по МНК\n",
    "lr_model_poly.fit(X_train_poly, y_train)\n",
    "#Делаем предсказание для тренировочной выборки\n",
    "y_train_predict_poly = lr_model_poly.predict(X_train_poly)\n",
    "#Делаем предсказание для тестовой выборки\n",
    "y_test_predict_poly = lr_model_poly.predict(X_test_poly)\n",
    " \n",
    "#Рассчитываем коэффициент детерминации для двух выборок\n",
    "print(\"Train R^2: {:.3f}\".format(metrics.r2_score(y_train, y_train_predict_poly)))\n",
    "print(\"Test R^2: {:.3f}\".format(metrics.r2_score(y_test, y_test_predict_poly)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Потрясающе! На тренировочной выборке коэффициент детерминации **R^2 = 0.929**, то есть наша модель описывает почти 93 % зависимости в данных.\n",
    "\n",
    "Смотрим на показатели тестовой выборки и сразу «спускаемся с небес на землю»: **R^2 = 0.268**. Метрика значительно ниже, чем на тренировочном наборе. Это и есть переобучение модели. Из-за своей сложности (количества факторов) модель полностью адаптировалась под тренировочные данные, но взамен получила высокий разброс в показателях на данных, которые она не видела ранее. \n",
    "\n",
    "Такая модель никому не нужна, так как она не отражает действительности.\n",
    "\n",
    "Однако не стоит расстраиваться — есть один замечательный метод, который сможет спасти нашу модель от переобучения, и это **регуляризация**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **РЕГУЛЯРИЗАЦИЯ**\n",
    "\n",
    "**Регуляризация** — способ уменьшения переобучения моделей машинного обучения.\n",
    "\n",
    "Идея регуляризации состоит в том, что мы намеренно пытаемся увеличить смещение модели, чтобы уменьшить разброс. Закон баланса в действии!\n",
    "\n",
    "Но как можно увеличить смещение модели? Мы можем «наказывать» модель за обучение сложным взаимосвязям. \n",
    "\n",
    "Математически это будет очень простая операция — добавление к функции потерь некоторого штрафа.\n",
    "\n",
    "**Штраф** — это дополнительное неотрицательное слагаемое в выражении для функции потерь, которое специально повышает ошибку.  За счёт этого слагаемого метод оптимизации (OLS или SGD) будет находить не истинный минимум функции потерь, а **псевдоминимум**.\n",
    "\n",
    "Есть несколько способов добавления штрафа к функции потерь:\n",
    "\n",
    "* L**1-регуляризация (Lasso)** — добавление к функции потерь суммы модулей коэффициентов, умноженных на коэффициент регуляризации ***α***:\n",
    "\n",
    "![](data\\f57.png)\n",
    "\n",
    "* **L2-регуляризация (Ridge)**, или регуляризация Тихонова — добавление к функции потерь суммы квадратов коэффициентов, умноженных на коэффициент регуляризации ***α***:\n",
    "\n",
    "![](data\\f58.png)\n",
    "\n",
    "Коэффициенты ***α*** (альфа) — это **коэффициенты регуляризации**. Они отвечают за то, насколько сильное смещение мы будем вносить в модель: чем оно больше, тем сильнее будет штраф за переобучение.\n",
    "\n",
    "А что по геометрии? Рассмотрим, как будет выглядеть минимум функции потерь в трёхмерном пространстве (вид сверху). Первый метод, L1, заставляет искать минимум функции потерь на пересечении его с ромбом, а второй, L2, — с окружностью. Визуализация представлена ниже:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/b62125f402f9f7558e5eab24cf3964bd/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml2-5_9.png)\n",
    "\n",
    "![](data\\sh.png)\n",
    "\n",
    "Примечание. В реализации sklearn для решения задачи оптимизации используется итеративный алгоритм [**координатного спуска**](https://ru.wikipedia.org/wiki/Координатный_спуск) (аналог градиентного спуска, но не использующий производную).\n",
    "\n",
    "Отличительной особенностью L1-регуляризации является то, что коэффициенты, которые соответствуют «ненужным», по мнению модели, факторам, обнуляются, то есть факторы просто не будут участвовать в предсказании. Это очень важно для сложных моделей, в обучении которых используются множество факторов (как в нашей модели выше — 91 фактор). Тем самым мы уменьшим сложность модели, сократим её разброс и, как следствие, уменьшим переобучение.\n",
    "\n",
    "В заключение теоретической части хочется отметить, что на практике никогда не ясно, какой из методов регуляризации сработает лучше всего. Выход — пробовать оба метода и сравнивать результаты.\n",
    "\n",
    "***\n",
    "\n",
    "Практика показывает, что обучение линейной регрессии с большим количеством признаков рекомендуется производить на **стандартизованных (нормализованных) данных**.\n",
    "\n",
    "Примечание. Часто возникает вопрос: как правильно проводить стандартизацию/нормализацию при наличии тренировочной и тестовой выборки?\n",
    "\n",
    "                                            Вы обучаете (fit()) преобразование на тренировочной выборке и используете его с одними и теми\n",
    "                                                        же параметрами на тренировочной и тестовой выборке (transform()).\n",
    "                                            Если производить подгонку на каждой из выборок в отдельности, вы внесёте смещение в модель.\n",
    "\n",
    "Стандартизацию (нормализацию) полезнее проводить перед генерацией полиномиальных признаков, иначе можно потерять масштаб полиномов.\n",
    "\n",
    "Давайте предобработаем наши данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(354, 104)\n",
      "(152, 104)\n"
     ]
    }
   ],
   "source": [
    "#Инициализируем стандартизатор StandardScaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "#Подгоняем параметры стандартизатора (вычисляем среднее и СКО)\n",
    "scaler.fit(X_train)\n",
    "#Производим стандартизацию тренировочной выборки\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "#Производим стандартизацию тестовой выборки\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    " \n",
    "#Создаём генератор полиномиальных признаков\n",
    "poly = preprocessing.PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly.fit(X_train_scaled)\n",
    "#Генерируем полиномиальные признаки для тренировочной выборки\n",
    "X_train_scaled_poly = poly.transform(X_train_scaled)\n",
    "#Генерируем полиномиальные признаки для тестовой выборки\n",
    "X_test_scaled_poly = poly.transform(X_test_scaled)\n",
    "#Выводим результирующие размерности таблиц\n",
    " \n",
    "print(X_train_scaled_poly.shape)\n",
    "print(X_test_scaled_poly.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* В sklearn методы регуляризации реализованы в классах **Lasso** (L1-регуляризация) и **Ridge*** (L2-регуляризация). Оба метода осуществляют поиск параметров с добавлением регуляризации. Процесс обучения и предсказания не отличается от обычной линейной регрессии.\n",
    "\n",
    "Давайте построим модель линейной регрессии с L1-регуляризацией на сгенерированных нами ранее полиномиальных признаках.\n",
    "\n",
    "Главный параметр инициализации Lasso — это **alpha**, коэффициент регуляризации. По умолчанию alpha=1. Практика показывает, что это довольно сильная регуляризация для L1-метода. Давайте установим значение этого параметра на 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R^2: 0.879\n",
      "Test R^2: 0.882\n"
     ]
    }
   ],
   "source": [
    "#Создаём объект класса линейной регрессии с L1-регуляризацией\n",
    "lasso_lr_poly = linear_model.Lasso(alpha=0.1)\n",
    "#Обучаем модель\n",
    "lasso_lr_poly.fit(X_train_scaled_poly, y_train)\n",
    "#Делаем предсказание для тренировочной выборки\n",
    "y_train_predict_poly = lasso_lr_poly.predict(X_train_scaled_poly)\n",
    "#Делаем предсказание для тестовой выборки\n",
    "y_test_predict_poly = lasso_lr_poly.predict(X_test_scaled_poly)\n",
    "#Рассчитываем коэффициент детерминации для двух выборок\n",
    "print(\"Train R^2: {:.3f}\".format(metrics.r2_score(y_train, y_train_predict_poly)))\n",
    "print(\"Test R^2: {:.3f}\".format(metrics.r2_score(y_test, y_test_predict_poly)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание на то, как изменились значения метрик. Да, на тренировочной выборке R^2 = 0.879. Метрика упала (до стандартизации + регуляризации значение R^2 было 0.929). Однако метрика ощутимо выросла на тестовой выборке: R^2 = 0.882 (ранее она была равна 0.268). Мы смогли преодолеть переобучение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.     0.    -0.038  0.    -0.523  2.766 -0.355 -0.605  0.    -0.595\n",
      " -0.763  0.    -3.259 -0.    -0.     0.     3.132 -0.141  0.     0.\n",
      "  0.    -0.     0.     0.    -0.015 -0.     0.063 -0.    -0.     0.\n",
      "  0.159 -0.    -0.    -0.     0.     0.07  -0.    -0.     0.017  0.\n",
      "  0.    -0.     0.     0.     0.     0.    -0.    -0.     0.     0.46\n",
      " -0.808 -0.643  0.    -0.    -0.     0.    -0.     0.    -0.43  -0.348\n",
      " -0.511 -0.     0.    -0.14  -0.    -0.277  0.    -0.     0.223 -0.\n",
      " -0.    -0.836 -0.054 -0.421  0.019 -0.784  0.    -0.     0.706  0.\n",
      " -0.    -0.335 -0.198  0.    -0.     0.     0.205 -0.     0.531 -0.\n",
      "  0.     0.048 -0.    -0.292  0.677  0.81  -0.    -1.151 -0.    -0.\n",
      " -0.    -0.288 -0.356  0.429]\n"
     ]
    }
   ],
   "source": [
    "# Давайте выведем значения коэффициентов модели, округлив их до третьего знака после запятой:\n",
    "print(np.round(lasso_lr_poly.coef_, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте на тех же данных обучим модель линейной регрессии с L2-регуляризацией. Для L2-регуляризации параметр alpha по умолчанию равен 1. Давайте попробуем использовать значение параметра alpha=10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R^2: 0.907\n",
      "Test R^2: 0.848\n"
     ]
    }
   ],
   "source": [
    "#Создаём объект класса линейной регрессии с L2-регуляризацией\n",
    "ridge_lr_poly = linear_model.Ridge(alpha=10)\n",
    "#Обучаем модель\n",
    "ridge_lr_poly.fit(X_train_scaled_poly, y_train)\n",
    "#Делаем предсказание для тренировочной выборки\n",
    "y_train_predict_poly = ridge_lr_poly.predict(X_train_scaled_poly)\n",
    "#Делаем предсказание для тестовой выборки\n",
    "y_test_predict_poly = ridge_lr_poly.predict(X_test_scaled_poly)\n",
    "#Рассчитываем коэффициент детерминации для двух выборок\n",
    "print(\"Train R^2: {:.3f}\".format(metrics.r2_score(y_train, y_train_predict_poly)))\n",
    "print(\"Test R^2: {:.3f}\".format(metrics.r2_score(y_test, y_test_predict_poly)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значения метрики R^2 на тренировочной и тестовой выборках для L2-регуляризации получились немного выше. В первую очередь мы всегда ориентируемся на тестовую выборку — это данные, которые модель ещё не видела.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.128 -0.049  0.084  0.117 -0.932  2.848 -1.008 -1.464  0.909 -0.908\n",
      " -0.653  0.971 -2.605  0.085 -0.032  0.466  2.721 -0.507  0.986  0.309\n",
      " -0.391 -0.714  0.376 -0.379  0.072  0.287  0.143 -0.138 -0.014  0.315\n",
      "  0.05  -0.409 -0.316  0.075  0.702  0.08  -0.281 -0.37   0.511  0.175\n",
      "  0.72   0.282  0.477  0.888 -0.012  0.074 -0.052  0.166 -0.263  0.414\n",
      " -1.129 -0.852  0.273  0.227 -0.106  0.368 -0.137 -0.241 -0.697 -0.177\n",
      " -0.326 -0.524  0.882 -0.637  0.344 -0.439 -0.006  0.386  0.233 -0.535\n",
      "  0.111 -0.802 -0.662 -0.56   0.22  -1.001  0.123  0.144  0.889 -0.114\n",
      " -0.086 -1.022 -0.71   1.08  -0.446 -0.178 -0.07  -0.496  0.874 -0.926\n",
      "  0.717  0.601 -0.49  -0.723  0.308  1.086 -0.448 -1.256  0.057  0.354\n",
      " -0.059 -0.433 -0.791  0.177]\n"
     ]
    }
   ],
   "source": [
    "# Давайте выведем значения коэффициентов модели, округлив их до третьего знака после запятой:\n",
    "print(np.round(ridge_lr_poly.coef_, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что L2-регуляризация не обнуляет коэффициенты — она использует для предсказания все признаки.\n",
    "\n",
    "Параметр alpha имеет очень важное значение: от его выбора зависит, как сильно мы будем штрафовать модель за переобучение. Важно найти значение, которое приносит наилучший эффект.\n",
    "\n",
    "Попробуйте вручную изменять параметр alpha для построенных ранее моделей. Согласитесь, это не очень удобно.\n",
    "***\n",
    "Давайте организуем процесс перебора параметров модели: создадим цикл, в котором будем перебирать 20 различных значений alpha в диапазоне от 0.001 до 1. Такой список проще всего создать с помощью функции [**linspace()**](https://pyprog.pro/array_creation/linspace.html) из библиотеки numpy.\n",
    "\n",
    "В цикле будем обучать модель линейной регрессии и L1-регуляризацией (Lasso), вычислять значения метрики R^2 на тренировочной и тестовой выборках и заносить результаты в списки train_scores и test_scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Создаём список из 20 возможных значений от 0.001 до 1\n",
    "alpha_list = np.linspace(0.001, 1, 20)\n",
    "#Создаём пустые списки, в которые будем добавлять результаты \n",
    "train_scores = []\n",
    "test_scores = []\n",
    "for alpha in alpha_list:\n",
    "    #Создаём объект класса линейной регрессии с L1-регуляризацией\n",
    "    lasso_lr_poly = linear_model.Lasso(alpha=alpha, max_iter=10000)\n",
    "    #Обучаем модель\n",
    "    lasso_lr_poly.fit(X_train_scaled_poly, y_train)\n",
    "    #Делаем предсказание для тренировочной выборки\n",
    "    y_train_predict_poly = lasso_lr_poly.predict(X_train_scaled_poly)\n",
    "    #Делаем предсказание для тестовой выборки\n",
    "    y_test_predict_poly = lasso_lr_poly.predict(X_test_scaled_poly)\n",
    "    #Рассчитываем коэффициенты детерминации для двух выборок и добавляем их в списки\n",
    "    train_scores.append(metrics.r2_score(y_train, y_train_predict_poly))\n",
    "    test_scores.append(metrics.r2_score(y_test, y_test_predict_poly))\n",
    "\n",
    "#Визуализируем изменение R^2 в зависимости от alpha\n",
    "fig, ax = plt.subplots(figsize=(12, 4)) #фигура + координатная плоскость\n",
    "ax.plot(alpha_list, train_scores, label='Train') #линейный график для тренировочной выборки\n",
    "ax.plot(alpha_list, test_scores, label='Test') #линейный график для тестовой выборки\n",
    "ax.set_xlabel('Alpha') #название оси абсцисс\n",
    "ax.set_ylabel('R^2') #название оси ординат\n",
    "ax.set_xticks(alpha_list) #метки по оси абсцисс\n",
    "ax.xaxis.set_tick_params(rotation=45) #поворот меток на оси абсцисс\n",
    "ax.legend(); #отображение легенды\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью графика мы можем подобрать оптимальное значение параметра alpha. Нам нужна такая точка на оси абсцисс, при которой на тестовой выборке наблюдается максимальная метрика и при этом разница между метриками на тренировочной и тестовой выборках минимальна.\n",
    "\n",
    "Видно, что R^2 на тестовой выборке достигает наибольшего значения в точке 0.0536. Причём в этой точке наблюдается примерное равенство метрик на каждом наборе данных. Далее метрика на тестовой выборке начинает падать.\n",
    "\n",
    "Обратите внимание, что на тренировочной выборке R^2 непрерывно падает с ростом alpha. Оно и понятно, ведь чем больше alpha, тем сильнее регуляризация и тем меньше модель подстраивается под обучающую выборку.\n",
    "\n",
    "Давайте подставим значение alpha=0.0536 в модель Lasso и получим результат:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R^2: 0.894\n",
      "Test R^2: 0.890\n"
     ]
    }
   ],
   "source": [
    "#Создаём объект класса линейной регрессии с L1-регуляризацией\n",
    "lasso_lr_poly = linear_model.Lasso(alpha=0.0536)\n",
    "#Обучаем модель \n",
    "lasso_lr_poly.fit(X_train_scaled_poly, y_train)\n",
    "#Делаем предсказание для тренировочной выборки\n",
    "y_train_predict_poly = lasso_lr_poly.predict(X_train_scaled_poly)\n",
    "#Делаем предсказание для тестовой выборки\n",
    "y_test_predict_poly = lasso_lr_poly.predict(X_test_scaled_poly)\n",
    "#Рассчитываем коэффициент детерминации для двух выборок\n",
    "print(\"Train R^2: {:.3f}\".format(metrics.r2_score(y_train, y_train_predict_poly)))\n",
    "print(\"Test R^2: {:.3f}\".format(metrics.r2_score(y_test, y_test_predict_poly)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примечание. Помимо основных методов регуляризации L1 и L2, существует комплексный метод.\n",
    "\n",
    "**Эластичная сетка (Elastic Net)** — это комбинация из двух методов регуляризации. Функция потерь в таком методе выглядит следующим образом:\n",
    "\n",
    "![](data\\f59.png)\n",
    "\n",
    "В sklearn реализация эластичной сетки находится в объекте класса [**ElasticNet**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html).\n",
    "\n",
    "Параметры ***α*** и ***λ*** позволяют регулировать вклад L1- и L2-регуляризации. На практике данный метод используется гораздо реже, так как нужно подбирать оптимальную комбинацию из двух параметров.\n",
    "\n",
    "Примечание. Регуляризация присутствует и в модели SGDRegressor, причём она используется по умолчанию. В инициализаторе данного класса есть параметр **penalty**, который позволяет управлять методом регуляризации. Параметр может принимать значения '**l1**', '**l2**' и '**elasticnet**'. По умолчанию используется L2-регуляризация (penalty='l2'). Коэффициент регуляризации (alpha) по умолчанию равен 0.0001 (относительно слабая регуляризация). Управляя двумя этими параметрами, вы можете настраивать тип регуляризации в SGD-методе и её «силу»."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
