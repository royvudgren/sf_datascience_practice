{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Дилемма смещения и разброса. Полиномиальные признаки. Регуляризация**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "# всё остальное\n",
    "from sklearn import linear_model\n",
    "import numpy as np #для матричных вычислений\n",
    "import pandas as pd #для анализа и предобработки данных\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt #для визуализации\n",
    "import seaborn as sns #для визуализации\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn') #установка стиля matplotlib\n",
    "\n",
    "from sklearn.datasets import load_boston "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **СМЕЩЕНИЕ И РАЗБРОС**\n",
    "\n",
    "Центральной проблемой всего обучения с учителем (не только линейных моделей) является ***дилемма смещения и разброса модели***. Давайте узнаем, что это такое.\n",
    "\n",
    "До этого момента мы обучали модели на всех имеющихся данных. С одной стороны, это имеет смысл, ведь мы хотим минимизировать ошибки модели, используя как можно больше данных для обучения.\n",
    "\n",
    "С другой стороны, из-за такого подхода становится труднее оценивать, насколько хорошо работает модель. Причина этого в том, что, если мы продолжим рассчитывать метрики, используя тренировочные данные, мы можем обнаружить, что при применении модели на незнакомых ей данных она работает довольно плохо.\n",
    "\n",
    "*Таким образом, модель может детально подстроиться под зависимость в обучающей выборке, но не уловить общей сути.*\n",
    "\n",
    "Такая проблема называется **переобучением** (overfitting). По сути, такая модель работает намного лучше с обучающими данными, чем с новыми. Она была чрезмерно натренирована на обнаружение уникальных характеристик обучающего набора данных, которые не являются общими закономерностями.\n",
    "\n",
    "**Недообучение** (underfitting) — проблема, обратная переобучению. Модель из-за своей слабости не уловила никаких закономерностей в данных. В этом случае ошибка будет высокой как для тренировочных данных, так и для данных, не показанных во время обучения.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/24d0e131092f71d843db3a5fe85cdbf5/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml2-5_1.png)\n",
    "\n",
    "* На первом рисунке изображена простая модель линейной регрессии, не способная уловить сложную зависимость в данных.\n",
    "* На втором рисунке изображена оптимальная модель, которая хорошо описывает зависимость и при этом не имеет переобучения (полином четвёртой степени).\n",
    "* На последнем рисунке изображен полином 27-й степени, который подстроился под каждую точку в тренировочном наборе, но не смог уловить общие закономерности.\n",
    "***\n",
    "С теоретической точки зрения недообучение и переобучение характеризуются понятиями **смещения** и **разброса** модели.\n",
    "***\n",
    "**Смещение (bias)** — это математическое ожидание разности между истинным ответом и ответом, выданным моделью. То есть это **ожидаемая ошибка модели**.\n",
    "\n",
    "![](data\\f49.png)\n",
    "\n",
    "В зарубежной литературе математическое ожидание часто обозначается как ***E***:\n",
    "\n",
    "![](data\\f50.png)\n",
    "\n",
    "Чем больше смещение, тем слабее модель. Если модель слабая, она не в состоянии выучить закономерность. Таким образом, налицо недообучение модели.\n",
    "***\n",
    "**Разброс (variance)** — это вариативность ошибки, то, насколько ошибка будет отличаться, если обучать модель на разных наборах данных. Математически это дисперсия (разброс) ответов модели.\n",
    "\n",
    "![](data\\f51.png)\n",
    "\n",
    "Примечание. В зарубежной литературе дисперсия часто обозначается как ***Var***:\n",
    "\n",
    "![](data\\f52.png)\n",
    "\n",
    "Чем больше разброс, тем больше ошибка будет колебаться на разных наборах данных. Наличие высокого разброса и есть свидетельство переобучения: модель подстроилась под конкретный набор данных и даёт высокий разброс ответов на разных данных.\n",
    "***\n",
    "Теоретически на составляющие смещения и разброса модели можно разложить любую функцию потерь. Например, разложение квадратичной ошибки (её математическое ожидание) будет выглядеть следующим образом:\n",
    "\n",
    "![](data\\f53.png)\n",
    "\n",
    "Математическое ожидание — это среднее значение во всей генеральной совокупности (на бесконечной выборке), а не на конкретных значениях. Это исключительно теоретическая величина.\n",
    "\n",
    "* ***σ*** — неустранимая ошибка, вызванная случайностью.\n",
    "* ***bias(y)^2*** — смещение модели (в квадрате).\n",
    "* ***variance(y)*** — разброс модели.\n",
    "\n",
    "**О чём нам говорит эта теоретическая формула?**\n",
    "\n",
    "Ошибка модели складывается из смещения модели (в квадрате) и её разброса, а также случайной ошибки. \n",
    "\n",
    "Если с последним слагаемым  мы ничего не сможем сделать, то вот на первые два (bias и variance) мы можем как-то повлиять. В идеале мы должны свести их к 0. Однако уменьшение одного слагаемого повлечёт увеличение другого. На практике часто приходится балансировать между смещёнными и нестабильными оценками.\n",
    "Дилемма смещения-дисперсии является центральной проблемой в обучении с учителем. В идеале мы хотим построить модель, которая точно описывает зависимости в тренировочных данных и хорошо работает на неизвестных данных. К сожалению, обычно это невозможно сделать одновременно.\n",
    "\n",
    "* **Усложняя** модель, мы пытаемся **уменьшить смещение (bias)**, однако появляется риск получить переобучение, то есть мы **повышаем разброс (variance)**. \n",
    "* С другой стороны, **снизить разброс (variance)** позволяют более **простые модели**, не склонные к переобучению, но есть риск, что простая модель не уловит зависимостей и окажется недообученной, то есть мы **повышаем смещение (bias)**.\n",
    "\n",
    "Если провести аналогию, что модель — это игрок в дартс, то самый лучший игрок будет иметь небольшое смещение и разброс: его дротики ложатся кучно «в яблочко». Если у игрока большое смещение, то дротики сгруппированы около другой точки, а если большой разброс — дротики разлетаются по всей мишени.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/c9e31e930c613a02ef6abd7c363158d6/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml2-5_2.png)\n",
    "***\n",
    "Теперь, когда мы знаем о теоретических основах проблемы переобучения и недообучения, что мы можем сделать, чтобы лучше судить о способности модели к обобщению на практике? Как диагностировать высокие bias и variance?\n",
    "\n",
    "Типичным решением является разделение данных на две части: **обучающий** и **тестовый** наборы. На тренировочном наборе данных мы будем обучать модель, подбирая параметры. Тестовый набор данных, который модель не видела при обучении, мы будем использовать для оценки истинного качества моделирования. Схематично можно представить это следующим образом:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/6b3371e3f1d290f0c216ac1e826005ff/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml2-5_3.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим, как это работает на практике. Работать будем с уже знакомыми нам данными — данными о домах в Бостоне."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\local_admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston \n",
    "\n",
    "boston = load_boston()\n",
    "#создаём DataFrame из загруженных numpy-матриц\n",
    "boston_data = pd.DataFrame(\n",
    "    data=boston.data, #данные\n",
    "    columns=boston.feature_names #наименования столбцов\n",
    ")\n",
    "#добавляем в таблицу столбец с целевой переменной\n",
    "boston_data['MEDV'] = boston.target\n",
    " \n",
    "#Составляем список факторов (исключили целевой столбец)\n",
    "features = boston_data.drop('MEDV', axis=1).columns\n",
    "#Составляем матрицу наблюдений X и вектор ответов y\n",
    "X = boston_data[features]\n",
    "y = boston_data['MEDV']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В **sklearn** для разделения выборки на тренировочную и тестовую есть функция [**train_test_split()**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) из модуля **model_selection**. Данная функция принимает следующие аргументы:\n",
    "\n",
    "* ***X*** и ***y*** — таблица с примерами и ответами к ним.\n",
    "* ***random_state*** — число, на основе которого генерируются случайные числа. Тренировочная и тестовая выборка генерируются случайно. Чтобы эксперимент был воспроизводимым, необходимо установить этот параметр в конкретное значение.\n",
    "* ***test_size*** — доля тестовой выборки. Параметр определяет, в каких пропорциях будет разделена выборка. Стандартные значения: 70/30, 80/20.\n",
    "\n",
    "Функция возвращает четыре объекта в следующем порядке: тренировочные примеры, тестовые примеры, тренировочные ответы и тестовые ответы. \n",
    "\n",
    "Итак, давайте разделим нашу выборку на тренировочную и тестовую в соотношении 70/30:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (354, 13) (354,)\n",
      "Test: (152, 13) (152,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#Разделяем выборку на тренировочную и тестовую в соотношении 70/30\n",
    "#Устанавливаем random_state для воспроизводимости результатов \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=40)\n",
    "#Выводим результирующие размеры таблиц\n",
    "print('Train:', X_train.shape, y_train.shape)\n",
    "print('Test:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После разделения в тренировочной выборке оказались 354 наблюдения, а в тестовой — 152.\n",
    "\n",
    "Затем обучим линейную регрессию (с помощью МНК) на тренировочных данных и рассчитаем R^2 для тренировочных и тестовых данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R^2: 0.743\n",
      "Test R^2: 0.722\n"
     ]
    }
   ],
   "source": [
    "#Создаём объект класса LinearRegression\n",
    "lr_model = linear_model.LinearRegression()\n",
    "#Обучаем модель по МНК\n",
    "lr_model.fit(X_train, y_train)\n",
    " \n",
    "#Делаем предсказание для тренировочной выборки\n",
    "y_train_predict = lr_model.predict(X_train)\n",
    "#Делаем предсказание для тестовой выборки\n",
    "y_test_predict = lr_model.predict(X_test)\n",
    " \n",
    "print(\"Train R^2: {:.3f}\".format(metrics.r2_score(y_train, y_train_predict)))\n",
    "print(\"Test R^2: {:.3f}\".format(metrics.r2_score(y_test, y_test_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, ***R^2 = 0.743*** на тренировочной выборке и ***R^2 = 0.722*** на тестовой выборке. То есть показатели довольно близки друг к другу (низкий разброс ответов модели для разных выборок).\n",
    "\n",
    "Это одно из свидетельств отсутствия переобучения. Это не удивительно, ведь линейная регрессия, построенная на 13 факторах, является довольно простой моделью: всего лишь 14 параметров, что очень мало по меркам машинного обучения. Риск переобучения возрастает с количеством факторов, которые участвуют в обучении модели.\n",
    "\n",
    "Но что насчёт смещения? Самый простой способ оценить смещение и недообученность модели — посмотреть на значение метрики и интуитивно оценить её.\n",
    "\n",
    "Может, наша модель слишком слабая? ***R^2 = 0.722*** — не слишком уж высокий показатель (напомним, максимум — 1). Возможно, стоит попробовать обучить более сложную модель. Например, можно построить модель полиномиальной регрессии.\n",
    "\n",
    "Примечание. Существуют более совершенные визуальные способы оценить наличие смещения и разброса, такие как кривая обучения и кросс-валидация. О них мы ещё поговорим далее в курсе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ПОЛИНОМИАЛЬНЫЕ ПРИЗНАКИ**\n",
    "\n",
    "**Полиномиальная регрессия (Polynomial Regression)** — это более сложная модель, чем линейная регрессия. Вместо уравнения прямой используется уравнение полинома (многочлена). Степень полинома может быть сколь угодно большой: чем больше степень, тем сложнее модель.\n",
    "\n",
    "В простом двумерном случае, когда мы рассматриваем зависимость целевого признака от одного фактора, полиномом второй степени будет уравнение параболы:\n",
    "\n",
    "![](data\\f55.png)\n",
    "\n",
    "Геометрически полином в двумерном пространстве — это некоторая кривая, которая пытается описать зависимость в данных. Выглядит это следующим образом:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/01a58ba6150976bb3c408323a9b5fbcf/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml2-5_5.png)\n",
    "\n",
    "Когда факторов больше одного, например два, то, помимо возведения фактора в квадрат, появляются ещё и комбинации из ***x1*** и ***x2***:\n",
    "\n",
    "![](data\\f56.png)\n",
    "\n",
    "Такая модель будет описывать сложную поверхность в трёхмерном пространстве, которая проставлена на рисунке:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/10616697357bc2252dd39b8fa419ffc0/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml2-5_6.png)\n",
    "\n",
    "Заметьте, как быстро растёт количество коэффициентов, а с ним и сложность модели. А ведь это только два фактора — ***x1*** и ***x2***. Мы не будем приводить уравнение для общего случая, как делали это с линейной регрессией, так как оно будет содержать слишком много слагаемых.\n",
    "\n",
    "Например, рассматривая построенный ранее график зависимости медианной цены домов от процента низкостатусного населения, вы могли заметить, что между данными показателями существует не линейная, а скорее полиномиальная связь.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/4b02bf0809fb707c3950ce0aa275368e/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml2-5_7.png)\n",
    "\n",
    "Заметим, что степени ***x*** можно тоже считать своего рода искусственными признаками в данных. Они называются полиномиальными признаками.\n",
    "\n",
    "Поэтому полиномиальная регрессия — это та же линейная регрессия, просто с новыми признаками. Полиномиальные признаки — один из самых распространённых методов FeatureEngineering.\n",
    "\n",
    "Пример:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/c9664ed6fac637130e49aa29ebfb67e9/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml2-5_8.png)\n",
    "\n",
    "Благодаря степенным слагаемым модель становится сложнее и начинает улавливать более сложные зависимости и выдавать меньшее смещение. Но, как вы понимаете, резко повышается риск переобучения модели — увеличивается разброс предсказаний на разных данных из-за количества факторов.\n",
    "***\n",
    "Давайте проследим за этим.\n",
    "\n",
    "Построить полиномиальную регрессию в sklearn очень просто. Для начала необходимо создать полиномиальные признаки с помощью объекта класса [**PolynomialFeatures**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) из модуля **preprocessing**. Это преобразователь, который позволит сгенерировать полиномиальные признаки любой степени и добавить их в таблицу. У него есть два важных параметра:\n",
    "\n",
    "* **degree** — степень полинома. По умолчанию используется степень 2.\n",
    "* **include_bias** — включать ли в результирующую таблицу столбец из единиц (x в степени 0). По умолчанию стоит True, но лучше выставить его в значение **False**, так как столбец из единиц и так добавляется в методе наименьших квадратов.\n",
    "\n",
    "*Примечание. Как правило, дата-сайентисты останавливаются на полиноме второй (максимум третьей) степени. Чем выше степень полинома, тем больше слагаемых, а значит, тем больше признаков и тем сложнее становится модель.*\n",
    "\n",
    "Для того чтобы подогнать генератор и рассчитать количество комбинаций степеней, мы используем метод **fit()**, а чтобы сгенерировать новую таблицу признаков, в которую будут включены полиномиальные признаки, используется метод **transform()**, в который нужно передать выборки:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(354, 104)\n",
      "(152, 104)\n"
     ]
    }
   ],
   "source": [
    "#Создаём генератор полиномиальных признаков\n",
    "poly = preprocessing.PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly.fit(X_train)\n",
    "#Генерируем полиномиальные признаки для тренировочной выборки\n",
    "X_train_poly = poly.transform(X_train)\n",
    "#Генерируем полиномиальные признаки для тестовой выборки\n",
    "X_test_poly = poly.transform(X_test)\n",
    "#Выводим результирующие размерности таблиц\n",
    "print(X_train_poly.shape)\n",
    "print(X_test_poly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# В результате мы получили два numpy-массива:\n",
    "print(type(X_train_poly))\n",
    "print(type(X_test_poly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R^2: 0.929\n",
      "Test R^2: 0.268\n"
     ]
    }
   ],
   "source": [
    "# Теперь попробуем скормить наши данные модели линейной регрессии, чтобы найти коэффициенты полинома по МНК-алгоритму:\n",
    "\n",
    "#Создаём объект класса LinearRegression\n",
    "lr_model_poly = linear_model.LinearRegression()\n",
    "#Обучаем модель по МНК\n",
    "lr_model_poly.fit(X_train_poly, y_train)\n",
    "#Делаем предсказание для тренировочной выборки\n",
    "y_train_predict_poly = lr_model_poly.predict(X_train_poly)\n",
    "#Делаем предсказание для тестовой выборки\n",
    "y_test_predict_poly = lr_model_poly.predict(X_test_poly)\n",
    " \n",
    "#Рассчитываем коэффициент детерминации для двух выборок\n",
    "print(\"Train R^2: {:.3f}\".format(metrics.r2_score(y_train, y_train_predict_poly)))\n",
    "print(\"Test R^2: {:.3f}\".format(metrics.r2_score(y_test, y_test_predict_poly)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Потрясающе! На тренировочной выборке коэффициент детерминации **R^2 = 0.929**, то есть наша модель описывает почти 93 % зависимости в данных.\n",
    "\n",
    "Смотрим на показатели тестовой выборки и сразу «спускаемся с небес на землю»: **R^2 = 0.268**. Метрика значительно ниже, чем на тренировочном наборе. Это и есть переобучение модели. Из-за своей сложности (количества факторов) модель полностью адаптировалась под тренировочные данные, но взамен получила высокий разброс в показателях на данных, которые она не видела ранее. \n",
    "\n",
    "Такая модель никому не нужна, так как она не отражает действительности.\n",
    "\n",
    "Однако не стоит расстраиваться — есть один замечательный метод, который сможет спасти нашу модель от переобучения, и это **регуляризация**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаём генератор полиномиальных признаков\n",
    "poly = preprocessing.PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly.fit(pd.dataframe())\n",
    "#Генерируем полиномиальные признаки для тренировочной выборки\n",
    "X_train_poly = poly.transform(X_train)\n",
    "#Генерируем полиномиальные признаки для тестовой выборки\n",
    "X_test_poly = poly.transform(X_test)\n",
    "#Выводим результирующие размерности таблиц\n",
    "print(X_train_poly.shape)\n",
    "print(X_test_poly.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8185abac01d49e395683ec62b5715351e7237a84c5f28c13c545c98638c429dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
