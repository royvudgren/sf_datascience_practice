{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ВВЕДЕНИЕ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом модуле мы продолжаем знакомство с моделями МО в области обучения с учителем. На этот раз поговорим о задаче классификации. Вспомним, где находится классификация на нашей карте машинного обучения:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/2c3b5a00129b4dbedff6ed9fd2995cdb/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml1-3_2.png)\n",
    "\n",
    "Вначале мы снова обратимся к классу линейных моделей и рассмотрим **логистическую регрессию**.\n",
    "\n",
    "↓\n",
    "\n",
    "Затем поговорим о **деревьях решений** для задачи классификации и научимся строить из этих деревьев целый лес."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Логистическая регрессия**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ранее мы обсуждали модель линейной регрессии, которая предназначена для решения задачи регрессии. Теперь нам предстоит разобраться с тем, как преобразовать данную модель, чтобы она решала задачу классификации.\n",
    "\n",
    "Для начала вспомним, что такое классификация.\n",
    "\n",
    "                                        Задача классификации (classification) — задача, в которой мы пытаемся предсказать\n",
    "                                        класс объекта на основе признаков в наборе данных. То есть задача сводится\n",
    "                                        к предсказанию целевого признака, который является категориальным.\n",
    "\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/f1c52bc11a0e81a47e3096dfb2750fdc/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml1-3_9.png)\n",
    "\n",
    "Когда **классов**, которые мы хотим предсказать, **только два**, классификация называется ***бинарной***. Например, мы можем предсказать, болен ли пациент раком, является ли изображение человеческим лицом, является ли письмо спамом и т. д.\n",
    "\n",
    "Когда **классов**, которые мы хотим предсказать, **более двух**, классификация называется ***мультиклассовой*** (многоклассовой). Например, предсказание модели самолёта по радиолокационным снимкам, классификация животных на фотографиях, определение языка, на котором говорит пользователь, разделение писем на группы.\n",
    "\n",
    "Для простоты мы пока разберёмся с бинарной классификацией, а в следующем юните обобщим результат на мультиклассовую.\n",
    "\n",
    "Что вообще означает **«решить задачу классификации»**? Это значит **построить разделяющую поверхность в пространстве признаков, которая делит пространство на части, каждая из которых соответствует определённому классу**. \n",
    "\n",
    "Ниже представлены примеры разделяющих поверхностей, которые производят бинарную классификацию. Красным и синим цветом обозначены классы, зелёным — собственно поверхность, которая делит пространство признаков на две части. В каждой из этих частей находятся только наблюдения определённого класса.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/295dc860fdc114b9449701a16f7b6200/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml3-2_1.png)\n",
    "\n",
    "Модели, которые решают задачу классификации, называются **классификаторами** (classifier).\n",
    "\n",
    "Если взять в качестве разделяющей поверхности некоторую плоскость (ровная поверхность на первом рисунке), то мы получаем модель логистической регрессии, которая тесно связана с рассмотренной нами ранее линейной регрессией.\n",
    "\n",
    "Давайте для начала вспомним, как выглядит уравнение модели линейной регрессии в общем случае:\n",
    "\n",
    "![](data\\f1.png)\n",
    "\n",
    "В общем случае это уравнение гиперплоскости, которая стремится приблизить зависимость целевой переменной от ***m*** факторов.\n",
    "\n",
    "![](data\\f2.png)\n",
    "\n",
    "**Но всё это работает только в том случае, когда целевой признак ***y***, который мы хотим предсказать, является числовым, например цена, вес, время аренды и т. д.**\n",
    "\n",
    "Что же делать с этой моделью, когда целевой признак  является категориальным? Например, является письмо спамом или обычным письмом?\n",
    "\n",
    "Можно предположить, что, раз у нас есть две категории, мы можем обозначить категории за **y=1** (Спам) и **y=0** (Не спам) и обучить линейную регрессию предсказывать 0 и 1.\n",
    "\n",
    "Но результат будет очень плохим. Выглядеть это будет примерно так:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/a41a936873f275372356f53254e213d5/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml3-2_2.png)\n",
    "\n",
    "Для больших значений ***x*** прямая будет выдавать значения больше 1, а для очень маленьких — меньше 0. Что это значит? Непонятно. Непонятно и то, что делать со значениями в диапазоне от 0 до 1. Да, можно относить значения на прямой выше 0.5 к классу 1, а меньше либо равным 0.5 — к классу 0, но это всё «костыли».\n",
    "\n",
    "Идея! Давайте переведём задачу классификации в задачу регрессии. Вместо предсказания класса будем предсказывать вероятность принадлежности к этому классу. \n",
    "\n",
    "Модель должна выдавать некоторую вероятность ***P***, которая будет определять, принадлежит ли данный объект к классу 1: например, вероятность того, что письмо является спамом. При этом вероятность того, что письмо является обычным письмом (класс 0), определяется как ***Q = 1 - P***.  \n",
    "\n",
    "Когда модель будет обучена на предсказание вероятности, мы зададим некоторый порог вероятности. Если предсказанная вероятность будет выше этого порога, мы определим объект к классу 1, а если ниже — к классу 0.\n",
    "\n",
    "Например, стандартный порог равен 0.5. То есть если вероятность ***P > 0.5***, мы будем считать письмо спамом, а если ***P <= 0.5*** — обычным информативным письмом.\n",
    "\n",
    "В итоге мы добьёмся того, что будем предсказывать не дискретный категориальный, а непрерывный числовой признак, который лежит в диапазоне [0, 1]. А это уже знакомая нам задача регрессии.\n",
    "\n",
    "→ Однако остался главный вопрос: как научить модель предсказывать вероятности, ведь они должны лежать строго в диапазоне от 0 до 1, а предсказания линейной регрессии лежат в диапазоне от +∞ до -∞? \n",
    "\n",
    "Тут-то мы и приходим к модели логистической регрессии — **регрессии вероятностей**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## **ОБЩЕЕ ПРЕДСТАВЛЕНИЕ О ЛОГИСТИЧЕСКОЙ РЕГРЕССИИ**\n",
    "\n",
    "Логистическая регрессия (Logistic Regression) — одна из простейших моделей для решения задачи классификации. Несмотря на простоту, модель входит в топ часто используемых алгоритмов классификации в Data Science.\n",
    "\n",
    "В основе логистической регрессии лежит логистическая функция (logistic function) ***σ(z)***  — отсюда и название модели. Однако более распространённое название этой функции — сигмόида (sigmoid). Записывается она следующим образом:\n",
    "\n",
    "![](data\\f3.png)\n",
    "\n",
    "А вот график её зависимости от аргумента ***z***:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/17e5df54243ab349c94672502f9f4dd0/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml3-2_3.png)\n",
    "\n",
    "В чём преимущество этой функции?\n",
    "\n",
    "У сигмоиды есть два очень важных для нас свойства:\n",
    "\n",
    "* Значения сигмоиды ***σ(z)*** лежат в диапазоне от 0 до 1 при любых значения аргумента ***z***: какой бы  вы ни подставили, число меньше 0 или больше 1 вы не получите.\n",
    "* Сигмоида выдаёт значения σ(z) < 0.5 при её аргументе z < 0, σ(z) > 0.5 — при z > 0 и σ(z) = 0.5 — при z = 0.\n",
    "\n",
    "Это ведь и есть свойства вероятности! Выходом сигмоиды является число от 0 до 1, которое можно интерпретировать как вероятность принадлежности к классу 1. Её мы и пытаемся предсказать.\n",
    "\n",
    "Основная идея модели логистической регрессии: возьмём модель линейной регрессии (обозначим её выход за ***z***)\n",
    "\n",
    "![](data\\f4.png)\n",
    "\n",
    "и подставим выход модели ***z*** в функцию сигмоиды, чтобы получить искомые оценки вероятности (**в математике принято писать оценочные величины с «шапкой» наверху, а истинные значения — без «шапки», это чистая формальность**):\n",
    "\n",
    "![](data\\f5.png)\n",
    "\n",
    "*Примечание*. Далее в модуле мы будем называть оценки вероятности ***P^*** просто вероятностью, но только для краткости. Это не значит, что эти оценки являются истинными вероятностями принадлежности к каждому из классов (их нельзя сосчитать, так как для этого нужна выборка бесконечного объёма). Если вы употребляете термин «вероятности» на собеседованиях, обязательно предварительно укажите, что вы подразумеваете оценку вероятности.\n",
    "\n",
    "Обучать будем всё в совокупности, пытаясь получить наилучшую оценку вероятности ***P^***. Если вероятность ***P^ > 0.5***, относим объект к классу 1, а если ***P^ <= 0.5***, относим объект к классу 0. \n",
    "\n",
    "Математически это записывается следующей формулой:\n",
    "\n",
    "![](data\\f6.png)\n",
    "\n",
    "Примечание. В данном выражении ***I[P^]*** называется индикаторной функцией. Она возвращает 1, если её значение больше какого-то порога, и 0 — в противном случае. Математики часто записывают просто квадратные скобки, опуская символ I: ***[P]***.\n",
    "\n",
    "Чего мы добились таким преобразованием?\n",
    "\n",
    "Если мы обучим модель, то есть подберём  коэффициенты w0, w1, w2, ... , wm (как их найти, обсудим чуть позже) таким образом, что для объектов класса 1 модель линейной регрессии начнёт выдавать положительное число, а для класса 0 — выдавать отрицательное число, то тогда, подставив предсказание линейной регрессии  в сигмоиду, мы сможем получать вероятности принадлежности к каждому из классов в диапазоне от 0 до 1.\n",
    "\n",
    "Далее по порогу вероятности мы сможем определять, к какому классу принадлежит объект.\n",
    "\n",
    "Это и есть наша цель. Мы свели задачу классификации к задаче регрессии для предсказания вероятностей. \n",
    "\n",
    "Для бинарной классификации описанное выше будет выглядеть следующим образом:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/8e663f443f23b393252ae85246b5d23f/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml3-2_4.png)\n",
    "***\n",
    "Рассмотрим, как это работает, на примере.\n",
    "\n",
    "Пусть мы каким-то образом обучили модель линейной регрессии предсказывать положительные числа для спам-писем и отрицательные — для обычных писем.\n",
    "\n",
    "Подаём характеристики письма x1, x2, ..., xm в выражение для линейной регрессии и получаем ответ модели, например ***z = 1.5***. Тогда, подставив его в сигмоиду, получим:\n",
    "\n",
    "![](data\\f7.png)\n",
    "\n",
    "Таким образом, вероятность того, что данный объект принадлежит классу спама, равна 0.82, что больше порогового значения 0.5. То есть мы относим данное письмо к спаму:  ***y^ = 1***.\n",
    "\n",
    "Пусть теперь мы подали на вход модели характеристики другого письма и получили ***z = -0.91***. Тогда, подставив этот результат в сигмоиду, получим:\n",
    "\n",
    "![](data\\f8.png)\n",
    "\n",
    "Вероятность того, что данный объект принадлежит классу спама, равна 0.28, что меньше порогового значения 0.5. Мы относим данное письмо к обычным письмам: ***y^ = 0***.\n",
    "\n",
    "Кстати, вероятность того, что это письмо будет обычным, равна противоположной вероятности: Q = 1 - 0.28 = 0.72. \n",
    "\n",
    "Полученное выражение для оценки вероятности ***P^*** и будет называться моделью логистической регрессии:\n",
    "\n",
    "![](data\\f9.png)\n",
    "\n",
    "***\n",
    "\n",
    "**Разберёмся с геометрией**\n",
    "\n",
    "Возьмём частный случай, когда класс объекта зависит от двух признаков — x1 и x2.\n",
    "\n",
    "Рассмотрим пример.\n",
    "\n",
    "Мы пытаемся предсказать поступление студента в университет в зависимости от результатов двух экзаменов. Целевой признак  — результат поступления в аспирантуру (admission outcome) с двумя возможными значениями: поступил или не поступил. Факторы: ***x1*** — результат сдачи первого экзамена (Exam1 Score) и ***x2*** — результат сдачи второго (Exam 2 Score). Будем предсказывать вероятность поступления с помощью логистической регрессии.\n",
    "\n",
    "Изобразим зависимость в пространстве двух факторов (вид сверху) в виде диаграммы рассеяния, а целевой признак отобразим в виде точек (непоступившие) и крестиков (поступившие).\n",
    "\n",
    "Если рассматривать уравнение линейной регрессии отдельно от сигмоиды, то геометрически построить логистическую регрессию на основе двух факторов — значит найти такие коэффициенты w0, w1, w2  уравнения плоскости, при которых наблюдается наилучшее разделение пространства на две части.\n",
    "\n",
    "![](data\\f10.png)\n",
    "\n",
    "Тогда выражение для ***z*** будет задавать в таком пространстве плоскость (в проекции вида сверху — прямую), которая разделяет всё пространство на две части. Над прямой вероятность поступления будет > 0.5 , а под прямой < 0.5:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/a8209513f0cfc3b554f494d79165269c/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml3-2_5.png)\n",
    "\n",
    "Кулинарная аналогия\n",
    "\n",
    "Возьмите по пригоршне риса и гречки и рассыпьте крупы на столе. Попытайтесь наложить лист бумаги вертикально на плоскость стола так, чтобы максимально качественно отделить виды круп друг от друга.\n",
    "\n",
    "Лист бумаги и будет разделяющей плоскостью. Вам необходимо найти такое расположение листа, при котором разделение будет наилучшим.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/77741a25b2f4998653c2af9b796d4bc8/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml3-2_6.png)\n",
    "\n",
    "Коэффициенты построенной выше плоскости равны (как их найти, обсудим позже):\n",
    "\n",
    "![](data\\f11.png)\n",
    "\n",
    "Тогда модель логистической регрессии будет иметь вид:\n",
    "\n",
    "![](data\\f12.png)\n",
    "\n",
    "Появляется новый абитуриент, и мы хотим предсказать вероятность его поступления. Баллы студента: x1 = 67, x2 = 53. Заметьте, что точка с такими координатами находится ниже нашей плоскости (то есть абитуриент, скорее всего, не поступит).\n",
    "\n",
    "Тогда:\n",
    "\n",
    "![](data\\f13.png)\n",
    "\n",
    "Итак, оценка вероятности поступления студента составляет 0.32, то есть его шанс поступления составляет 32 %.\n",
    "\n",
    "А что если мы возьмём точку, лежащую выше прямой?\n",
    "\n",
    "Например, появился абитуриент с баллами x1 = 80, x2 = 75. Подставим его баллы в нашу модель логистической регрессии, чтобы понять, какова оценочная вероятность поступления:\n",
    "\n",
    "![](data\\f15.png)\n",
    "\n",
    "Таким образом, оценка вероятности поступления абитуриента составляет 0.99, шанс поступления — 99 %.\n",
    "\n",
    "**В чём математический секрет?**\n",
    "\n",
    "Математически подстановка в уравнение плоскости точки, которая не принадлежит ей (находится ниже или выше), означает вычисление расстояния от этой точки до плоскости.\n",
    "\n",
    "Если точка находится ниже плоскости, расстояние будет отрицательным (z < 0).\n",
    "Если точка находится выше плоскости, расстояние будет положительным (z > 0).\n",
    "Если точка находится на самой плоскости, (z = 0).\n",
    "Мы знаем, что подстановка отрицательных чисел в сигмоиду приведёт к вероятности P^ < 0.5, а постановка положительных — к вероятности P^ > 0.5. \n",
    "\n",
    "Таким образом, ключевым моментом в предсказании логистической регрессии является расстояние от точки до разделяющей плоскости в пространстве факторов. Это расстояние в литературе часто называется **отступом (margin)**. \n",
    "\n",
    "В этом и состоит секрет работы логистической регрессии.\n",
    "\n",
    "Чем больше расстояние от точки, находящейся выше разделяющей плоскости, до самой плоскости, тем больше оценка вероятности принадлежности к классу 1.\n",
    "\n",
    "Попробуйте подставить различные координаты точек в модель логистической регрессии и убедитесь в этом.\n",
    "\n",
    "Можно построить тепловую карту, которая показывает, чему равны вероятности в каждой точке пространства:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/30baeed1eeb22b64aa28d75952115e87/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml3-2_7.png)\n",
    "\n",
    "На рисунке точки, которые относятся к классу непоступивших абитуриентов, лежащие ниже разделяющей плоскости, находятся в красной зоне. Чем насыщеннее красный цвет, тем ниже вероятность того, что абитуриент поступит в аспирантуру. И наоборот, точки, которые относятся к классу поступивших абитуриентов, лежащие выше разделяющей плоскости, находятся в синей зоне. Чем насыщеннее синий цвет, тем выше вероятность того, что абитуриент поступит в аспирантуру.\n",
    "***\n",
    "\n",
    "Для случая зависимости целевого признака y от трёх факторов x1, x2 и x3, например от баллов за два экзамена и рейтинга университета, из которого выпустился абитуриент, выражение для z будет иметь вид:\n",
    "\n",
    "![](data\\f16.png)\n",
    "\n",
    "Уравнение задаёт плоскость в четырёхмерном пространстве. Но если вспомнить, что  — категориальный признак и классы можно обозначить цветом, то получится перейти в трёхмерное пространство. Разделяющая плоскость будет выглядеть следующим образом:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/ecda661b626060bcc2c4dc6593249013/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml3-2_8.png)\n",
    "\n",
    "В общем случае, когда у нас есть зависимость от  факторов, линейное выражение, находящееся под сигмоидой, будет обозначать разделяющую гиперплоскость.\n",
    "\n",
    "![](data\\f17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ПОИСК ПАРАМЕТРОВ ЛОГИСТИЧЕСКОЙ РЕГРЕССИИ**\n",
    "\n",
    "Итак, мы разобрались с тем, как выглядит модель логистической регрессии и что она означает в геометрическом смысле.\n",
    "\n",
    "Но остался один главный вопрос: как найти такие коэффициенты w = (w0, w1, w2, ... , wm), чтобы гиперплоскость разделяла пространство наилучшим образом?\n",
    "\n",
    "Вновь обратимся к нашей схеме минимизации эмпирического риска:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/5fac5fe11d423f674949523e3db643c9/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml1-2_1.png)\n",
    "\n",
    "Можно предположить, что стоит использовать метод наименьших квадратов. Введём функцию ошибки — средний квадрат разности MSE между истинными классами  и предсказанными классами  и попытаемся его минимизировать.\n",
    "\n",
    "Сразу можно достоверно предсказать, что результат такого решения будет плохим, поэтому воздержимся от его использования.\n",
    "\n",
    "Здесь нужен другой подход. Это метод максимального правдоподобия (Maximum Likelihood Estimation — MLE). \n",
    "\n",
    "**Правдоподобие** — это оценка того, насколько вероятно получить истинное значение целевой переменной ***y*** при данных ***x*** и параметрах ***w***. \n",
    "\n",
    "Данный метод позволяет получить функцию правдоподобия.\n",
    "\n",
    "Цель метода — найти такие параметры w = (w0, w1, w2, ... , wm), в которых наблюдается максимум функции правдоподобия. Подробнее о выводе формулы вы можете прочитать [**здесь**](https://habr.com/ru/post/485872/).\n",
    "\n",
    "А мы пока что опустим математические детали метода и приведём только конечную формулу:\n",
    "\n",
    "![](data/f18.PNG)\n",
    "\n",
    "Не пугайтесь. Давайте разберёмся, что есть что и как работает эта функция.\n",
    "\n",
    "* **n** — количество наблюдений.\n",
    "* **yi** — это истинный класс (1 или 0) для -ого объекта из набора данных.\n",
    "* **P^i = σ (zi)** — предсказанная с помощью логистической регрессии вероятность принадлежности к классу 1 для i-ого объекта из набора данных.\n",
    "* **zi** — результат подстановки i-ого объекта из набора данных в уравнение разделяющей плоскости **zi = w * xi**.\n",
    "* log — логарифм (обычно используется натуральный логарифм по основанию e - ln).\n",
    "\n",
    "![](data/f19.PNG)\n",
    "\n",
    "Такие расчёты можно производить для любых значений параметров, меняется только оценка вероятности **P^i**.\n",
    "\n",
    "*Примечание. К сожалению, функция likelihood не имеет интерпретации, то есть нельзя сказать, что значит число 2.34 в контексте правдоподобия.*\n",
    "\n",
    "**Цель — найти такие параметры, при которых наблюдается максимум этой функции.**\n",
    "\n",
    "Теперь пора снова применить магию математики, чтобы привести задачу к привычному нам формату минимизации эмпирического риска. По правилам оптимизации, если поставить перед функцией минус, то задача оптимизации меняется на противоположную: был поиск максимума — станет поиском минимума.\n",
    "\n",
    "Таким образом мы получим функцию потерь ***L(w)***, которая носит название **«функция логистических потерь»**, или **logloss**, или **кросс-энтропия** (cross-entropy loss):\n",
    "\n",
    "![](data/f20.PNG)\n",
    "\n",
    "Вот эту функцию мы и будем минимизировать в рамках поиска параметров логистической регрессии. Мы должны найти такие параметры разделяющей плоскости  w, при которых наблюдается минимум logloss.\n",
    "\n",
    "Знакомая задача? Всё то же самое, что и с линейной регрессией, только функция ошибки другая.\n",
    "\n",
    "→ К сожалению, для такой функции потерь аналитическое решение оптимизационной задачи найти не получится: при расчётах получается, что его попросту не существует.\n",
    "\n",
    "Но мы помним, что, помимо аналитических решений, есть и численные.\n",
    "\n",
    "Например, для поиска параметров можно использовать знакомый нам градиентный спуск. Вспомним, как выглядит итерационная формула данного метода:\n",
    "\n",
    "![](data/f21.PNG)\n",
    "\n",
    "Повторим её смысл: новое значение параметров ***w^(k+1)*** получается путём сдвига текущих ***w^(k)*** в сторону вектора антиградиента - ***∇L(w^(k))***, умноженного на темп обучения ***η***.\n",
    "\n",
    "Математическую реализацию вычисления градиента для logloss мы обсудим далее в курсе, а пока нас интересует исключительно его смысл.\n",
    "\n",
    "Мы уже знаем, что для того, чтобы повысить шанс пройти мимо локальных минимумов функции потерь, используется не сам градиентный спуск, а его модификации: например, можно использовать уже знакомый нам стохастический градиентный спуск (SGD).\n",
    "\n",
    "Помним, что применение градиентного спуска требует предварительного масштабирования данных (стандартизации/нормализации). В реализации логистической регрессии в sklearn предусмотрено ещё несколько методов оптимизации, для которых масштабирование не обязательно. О них мы упомянём в практической части модуля.\n",
    "\n",
    "Во избежание переобучения модели в функцию потерь логистической регрессии традиционно добавляется регуляризация. В реализации логистической регрессии в sklearn она немного отличается от той, что мы видели ранее для линейной регрессии.\n",
    "\n",
    "При L1-регуляризации мы добавляем в функцию потерь L(w) штраф из суммы модулей параметров, а саму функцию logloss умножаем на коэффициент ***C***:\n",
    "\n",
    "![](data/f22.PNG)\n",
    "\n",
    "А при L2-регуляризации — штраф из суммы квадратов параметров:\n",
    "\n",
    "![](data/f23.PNG)\n",
    "\n",
    "Значение коэффициента ***C*** — коэффициент, обратный коэффициенту регуляризации. Чем больше ***C***, тем меньше «сила» регуляризации.\n",
    "\n",
    "***\n",
    "\n",
    "Предлагаем вам посмотреть на то, как будет меняться форма сигмоиды, разделяющей плоскости при минимизации функции потерь logloss (она обозначена как cross-entropy в виде концентрических кругов — вид сверху), с помощью обычного градиентного спуска (не стохастического) в виде анимации.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/bfdf530906dbc60202d51e4233e10185/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml3-2_12.gif)\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/dd3061bf76c343ce85ae577b688d7be9/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml3-2_13.gif)\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/dd3061bf76c343ce85ae577b688d7be9/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml3-2_14.gif)\n",
    "\n",
    "Не волнуйтесь, все громоздкие формулы уже реализованы в классических библиотеках, таких как sklearn. Но нам важно понимать принцип того, что происходит «под капотом», чтобы верно интерпретировать результат и по возможности управлять им.\n",
    "\n",
    "Теперь давайте перейдём к практической реализации логистической регрессии и решим с её помощью задачу классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ЛОГИСТИЧЕСКАЯ РЕГРЕССИЯ В SKLEARN**\n",
    "\n",
    "Мы будем работать со знакомым вам по модулю «Очистка данных» наборе данных о диабете, первоначально полученном в Национальном институте диабета, болезней органов пищеварения и почек.\n",
    "\n",
    "Наша цель будет состоять в том, чтобы диагностически предсказать, есть ли у пациента диабет. На выбор экземпляров из более крупной базы данных было наложено несколько ограничений. В частности, все пациенты здесь — женщины не моложе 21 года из индейского племени Пима.\n",
    "\n",
    "В модуле по очистке данных вы уже производили очистку этого набора данных:\n",
    "\n",
    "* удалили дубликаты,\n",
    "* удалили неинформативный признак Gender,\n",
    "* обработали «скрытые» пропуски в данных,\n",
    "* избавились от потенциальных выбросов.\n",
    "\n",
    "Очищенный датасет вы можете найти [здесь](https://lms.skillfactory.ru/assets/courseware/v1/c21dd892cc0cc4fba3976b2a91559ff0/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/diabetes_cleaned.zip).\n",
    "\n",
    "Итак, импортируем библиотеки, необходимые нам для работы с данными:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #для матричных вычислений\n",
    "import pandas as pd #для анализа и предобработки данных\n",
    "import matplotlib.pyplot as plt #для визуализации\n",
    "import seaborn as sns #для визуализации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>98.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.430</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>112.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>35.7</td>\n",
       "      <td>0.148</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>108.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>30.8</td>\n",
       "      <td>0.158</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>107.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>24.6</td>\n",
       "      <td>0.856</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>136.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>29.9</td>\n",
       "      <td>0.210</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness   BMI  \\\n",
       "0            6     98.0           58.0           33.0  34.0   \n",
       "1            2    112.0           75.0           32.0  35.7   \n",
       "2            2    108.0           64.0           29.0  30.8   \n",
       "3            8    107.0           80.0           29.0  24.6   \n",
       "4            7    136.0           90.0           29.0  29.9   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.430   43        0  \n",
       "1                     0.148   21        0  \n",
       "2                     0.158   21        0  \n",
       "3                     0.856   34        0  \n",
       "4                     0.210   50        0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_data = pd.read_csv('data/diabetes_cleaned.csv')\n",
    "diabetes_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8185abac01d49e395683ec62b5715351e7237a84c5f28c13c545c98638c429dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
