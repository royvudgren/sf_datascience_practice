{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Виды машинного обучения: обучение без учителя**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомним нашу карту машинного обучения, а точнее — её часть, описывающую обучение без учителя:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/d59b6e3e1f88c0037c042012f60b1124/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml1-4_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## **ОБУЧЕНИЕ БЕЗ УЧИТЕЛЯ**\n",
    "\n",
    "Разметка данных — роскошь. Например, если мы хотим написать классификатор автобусов, неужели нам придётся идти на улицу и фотографировать миллион автобусов и подписывать, где какой? Согласитесь, не очень приятное занятие: мы учимся на курсе Data Science не для того, чтобы размечать картинки в редакторе вручную.\n",
    "\n",
    "Когда нет разметки, конечно, есть надежда на фрилансеров, которые готовы сделать вашу работу за вас, но это не бесплатно. На самом деле так обычно и поступают на практике. \n",
    "\n",
    "Однако можно попробовать применить обучение без учителя. Обучение без учителя подразумевает, что у вас нет правильных ответов. То есть **признак, который вы хотите предсказать, вам недоступен**. Подход основан на том, что алгоритм самостоятельно выявляет зависимости в данных только на основе схожести объектов в данных между собой.\n",
    "\n",
    "Данный вид машинного обучения разбивается на несколько самостоятельных типов задач:\n",
    "\n",
    "* **кластеризация** (с помощью методов кластеризации мы можем разделить наши объекты на группы (кластеры) и попытаться проанализировать эти группы)\n",
    "* **понижение размерности** (С помощью методов понижения размерности мы можем отбросить ненужные признаки и выжать из данных только самые важные знания)\n",
    "* **ассоциация** (методы ассоциации помогают находить закономерности в последовательных действиях)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **КЛАСТЕРИЗАЦИЯ**\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/db1a45caf3442f690b61a292b6a01147/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml1-4_2.png)\n",
    "\n",
    "Задача **кластеризации (clustering)** — это задача, в которой мы **разделяем данные на группы** на основе признаков в данных.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/71ed6695ea8ac1cb291898658a7a9e98/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml1-4_3.png)\n",
    "\n",
    "На первый взгляд может показаться, что эта задача идентична классификации, но в задаче кластеризации у нас нет правильных ответов. То есть у нас нет учителя: мы пытаемся разделить данные на категории, опираясь только на **«близость»** объектов. Причём результаты каждого из алгоритмов кластеризации могут быть разными, потому что «близость» — понятие относительное и её можно измерять различными способами.\n",
    "***\n",
    "Примеры использования кластеризации: сегментация рынка на категории, объединение близких точек на карте, разделение клиентов по уровню платёжеспособности, кластеризация студентов по их интересам или обучаемости, анализ и разметка новых данных.\n",
    "\n",
    "Ещё один отличный пример — маркеры на Google Картах. Когда вы ищете все крафтовые бары в Москве, приложение группирует их в кружочки с цифрой — при попытке нарисовать миллион маркеров приложение просто зависло бы. Критерием объединения маркеров является расстояние между ними.\n",
    "\n",
    "Более сложные примеры — приложения iPhoto или Google Фото, которые находят лица людей на фотографиях и группируют их в альбомы. Приложение не знает, как зовут ваших друзей, но может отличить их по характерным чертам лица. Это типичная кластеризация.\n",
    "***\n",
    "**Цель обучения — построить модель, которая наилучшим образом объединит «похожие» объекты в группы.**\n",
    "\n",
    "Количество кластеров в зависимости от выбранного алгоритма можно задать заранее или доверить их количество алгоритму.\n",
    "\n",
    "На рисунке ниже представлен пример кластеризации товаров по двум признакам: цена товара и его популярность. Количество кластеров, которое нашёл алгоритм, — 5.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/8230dea92e94710674752e68431f00e3/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml1-4_4.png)\n",
    "\n",
    "Самая сложная часть в задаче кластеризации — проинтерпретировать полученные кластеры, понять, что именно объединяет объекты, попавшие в тот или иной кластер.\n",
    "\n",
    "Типичные методы кластеризации при известном заранее количестве кластеров:\n",
    "\n",
    "* **метод k-средних** (k-means),\n",
    "* **EM-алгоритм**,\n",
    "* **агломеративная кластеризация**.\n",
    "\n",
    "Иногда кластеры могут быть очень сложными и вы можете не знать их количество.\n",
    "\n",
    "Представьте, что вы — геолог, которому нужно найти на карте схожие по структуре горные породы: ваши кластеры не только будут вложены друг в друга, но вы также не знаете, сколько их вообще получится.\n",
    "\n",
    "Для таких задач используются хитрые методы, например алгоритм **DBSCAN**. Он сам находит скопления точек и строит вокруг кластеры.\n",
    "\n",
    "Математическая постановка задачи кластеризации будет выглядеть следующим образом:\n",
    "\n",
    "![](data\\f4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ПОНИЖЕНИЕ РАЗМЕРНОСТИ (ОБОБЩЕНИЕ)**\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/0677881186df28f2a038657cc8696c87/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml1-4_5.png)\n",
    "\n",
    "**Понижение размерности (dimensionality reduction)** — задача, в которой мы пытаемся уменьшить количество признаков, характеризующих объект. Обычно мы уменьшаем количество признаков до 2-3 для того, чтобы получить возможность визуализировать данные.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/71fe5669b5c2bdf838ab17d46c0ec811/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml1-4_6.png)\n",
    "\n",
    "**Для чего это нужно?**\n",
    "\n",
    "Мы уже с вами не раз работали с данными, которые характеризуются большим количеством признаков: вспомните данные о ценах на дома в Мельбурне, о квартирах в Москве, о вакансиях на hh.ru или о качестве вин. \n",
    "\n",
    "Это вполне стандартная ситуация — целевой признак зависит от множества других признаков. Нам бы хотелось отобразить комплексную зависимость (от всех признаков одновременно) на графике. Но проблема в том, что для визуализации нам просто не хватит числовых осей!\n",
    "\n",
    "Выход из этой ситуации — использовать методы понижения размерности.\n",
    "\n",
    "Практическая польза этих методов состоит в том, что мы можем **объединить несколько признаков в один и получить абстракцию**.\n",
    "\n",
    "Например, собаки с треугольными ушами, длинными носами и большими хвостами объединяются в полезную абстракцию «овчарки». Да, мы теряем информацию о конкретных овчарках, но новая абстракция точно полезнее этих лишних деталей.\n",
    "\n",
    "Ещё одно проявление пользы таких алгоритмов — **увеличение скорости обучения**. Мы уже говорили о проклятии размерности: «чем больше признаков в данных, тем сложнее модели обучиться на них». Методы понижения размерности позволяют свести эту проблему к минимуму. Однако точность моделей может сильно упасть, поэтому необходимо уметь находить баланс.\n",
    "\n",
    "Ещё один приятный бонус — мы автоматически **избавляемся от мультиколлинеарности признаков**. Методы понижения размерности устроены так, что в первую очередь объединяют между собой наиболее коррелированные признаки.\n",
    "***\n",
    "Примеры использования методов понижения размерности: визуализация, рекомендательные системы, определение тематик и поиск похожих между собой документов, анализ фейковых изображений.\n",
    "\n",
    "Инструмент на удивление хорошо подошёл для определения **тематик текстов (Topic Modelling)**. Благодаря понижению размерности можно абстрагироваться от конкретных слов до уровня смыслов даже без привлечения учителя со списком категорий. Алгоритм назвали **[«латентно-семантический анализ» (LSA)](https://habr.com/post/110078/)**, и его идея состоит в том, что частота появления слова в тексте зависит от его тематики: в научных статьях больше технических терминов, в новостях о политике — имён политиков.\n",
    "\n",
    "Другое популярное применение метода уменьшения размерности — **рекомендательные системы**. Оказалось, если обобщать оценки пользователей, получается неплохая система рекомендаций кино, музыки, игр и чего угодно вообще. У Яндекса есть хорошая **[лекция](https://habr.com/ru/company/yandex/blog/241455/)**, посвящённая данной тематике. \n",
    "***\n",
    "Цель обучения — построить модель, которая переводит пространство признаков из размерности  в размерность , при этом сохранив наибольший объём информации. Математически это записывается как ![](data\\f5.png)\n",
    "\n",
    "На рисунке ниже представлен пример понижения размерности данных с двух признаков до одного. Используемый алгоритм — метод главных компонент (Principal Component Analysis, PCA). С помощью него находится обобщающая ось (компонента), которая содержит наибольшее количество информации о признаках. Наблюдения проецируются на новую ось, в результате чего получается новый признак, который является обобщением двух предыдущих. Обучение состоит в поиске оптимальной оси обобщения, которая содержит наибольший объём информации.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/16582c65680965ad178507594f1dca06/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml1-4_8.png)\n",
    "\n",
    "Основные алгоритмы понижения размерности:\n",
    "\n",
    "* **метод главных компонент (PCA)**,\n",
    "* **сингулярное разложение (SVD)**,\n",
    "* **латентное размещение Дирихле (LDA)**,\n",
    "* **латентный семантический анализ (LSA)**,\n",
    "* **t-SNE**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **АССОЦИАЦИЯ (БОНУС)**\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/04c801dd49a3f49a891ac0ecd5550acc/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml1-4_9.png)\n",
    "\n",
    "**Ассоциация (association)** — это задача, в которой мы пытаемся найти правила и законы, по которым существует последовательность действий. Давайте сразу перейдём к примерам.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/9745e79b6b34681343e74a878943d537/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml1-4_10.png)\n",
    "\n",
    "Предположим, покупатель берёт в дальнем углу магазина мясо и идёт на кассу. Стоит ли ставить на его пути пакетики с сухой смесью для этого мяса? Часто ли люди берут их вместе? Мясо с приправами, вероятно, да. Но какие ещё товары покупают вместе? Когда вы владелец сети гипермаркетов, ответ для вас не всегда очевиден, но одно тактическое улучшение в расстановке товаров может принести хорошую прибыль.\n",
    "\n",
    "Мы можем создавать различные шаблоны, такие как определённые группы предметов, которые постоянно покупаются вместе, предметы, похожие на предметы, которые вы просматриваете, и т. д. Это, в свою очередь, помогает в правильном размещении предметов на веб-сайте или внутри физического заведения.\n",
    "\n",
    "**Примеры использования: прогноз акций и распродаж, анализ товаров, покупаемых вместе, расстановка товаров на полках, анализ паттернов поведения на веб-сайтах.**\n",
    "\n",
    "Основные ассоциативные модели:\n",
    "\n",
    "* **Apriori**,\n",
    "* **Eclat**,\n",
    "* **FP Growth**.\n",
    "\n",
    "На сегодняшний день данная область машинного обучения является наименее популярной и медленно развивающейся. Мы не будем детально рассматривать задачу ассоциации в нашем курсе, так как область её применения довольно специфичная и узконаправленная.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
