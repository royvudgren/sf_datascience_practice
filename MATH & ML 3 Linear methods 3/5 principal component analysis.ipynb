{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Метод главных компонент**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте для начала вспомним, в чём заключается задача снижения размерности.\n",
    "***\n",
    "* **Задача снижения размерности** — это задача преобразования данных с целью уменьшения количества признаков, которые описывают объект.\n",
    "***\n",
    "\n",
    "Основными целями снижения размерности являются:\n",
    "\n",
    "* Сокращение времени работы моделей машинного обучения.\n",
    "* Сокращение избыточной информации за счёт выделения наиболее влиятельных факторов.\n",
    "* Подготовка данных для визуализации.\n",
    "\n",
    "Мы знаем, что снижать размерность можно как **линейными**, так и **нелинейными** способами.\n",
    "\n",
    "В этом модуле мы приведём обзор именно линейных методов снижения размерности. Начнём мы с **метода главных компонент (Principal Compoment Analysis, PCA)**.\n",
    "\n",
    "Мы кратко познакомились с ним, когда говорили о задачах обучения без учителя в модуле ML-4. Обучение с учителем: кластеризация и техники понижения размерности. А сейчас посмотрим, на каких принципах линейной алгебры работает данный метод."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## **ПОСТАНОВКА ЗАДАЧИ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Допустим, мы хотим прогнозировать целевую переменную y по двум факторам: x1 и x2. В качестве модели мы можем использовать всё что угодно, но для конкретики мы возьмём модель линейной регрессии:\n",
    "\n",
    "![](data/6.PNG)\n",
    "\n",
    "Вспомним классический датасет о домах в Бостоне (Boston Housing Dataset). \n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/8b30df3d6d9815688c7069143b5d7e1d/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst-math-ml-3-14.png)\n",
    "\n",
    "Рассмотрим в качестве признаков DIS и NOX — это усреднённое расстояние до Employment Centres и уровень загрязнения воздуха. \n",
    "\n",
    "                Зададимся вопросом: верно ли, что взять оба признака лучше, чем один?\n",
    "\n",
    "Два признака, как правило, содержат больше информации, чем один. Однако часто бывает, что они сильно скоррелированы. Это значит, что при построении оценки вектора весов линейной регрессии по классическому МНК:\n",
    "\n",
    "![](data/7.PNG)\n",
    "\n",
    "…мы можем получить плохо обусловленную матрицу Грама A.T@A.\n",
    "\n",
    "Мы не умеем идеально точно считать на компьютере, поэтому получим огромные ошибки из-за округлений. Получается, что хотя мы неизбежно теряем часть информации при выборе только одного признака, с вычислительной точки зрения это более выгодная стратегия.\n",
    "\n",
    "Итак, мы хотим выбрать один фактор из двух, который даст нам лучшие регрессии.\n",
    "\n",
    "На тепловой карте видно, что корреляция между нашими признаками DIS и NOX достаточно велика и составляет –0.77:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/d00a001119c5ca0186de9b90921b1e11/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst-math-ml-3-15.png)\n",
    "\n",
    "Это значит, что в районах, которые расположены ближе к Employment Centres, выше уровень загрязнения воздуха. \n",
    "\n",
    "На следующей картинке вы видите диаграмму рассеивания по этим двум признакам, а также линейную регрессию, построенную для зависимости фактора DIS от фактора NOX:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/5b2aa301f03e1fa178051fab35c5570a/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst-math-ml-3-16.png)\n",
    "***\n",
    "Какой же фактор выбрать: NOX или DIS?\n",
    "\n",
    "Ни тот, ни другой! Наилучшим новым признаком окажется линейная комбинация из старых факторов, расположенная вдоль синей прямой.\n",
    "\n",
    "Такая линейная комбинация называется **главной компонентой**. А алгоритм поиска этой комбинации как раз и называется **методом главных компонент**. Давайте приведём общую структуру алгоритма для случая двух факторов:\n",
    "\n",
    "1. Составить корреляционную матрицу факторов  C. Она же — матрица Грама стандартизированных факторов:\n",
    "\n",
    "![](data/8.PNG)\n",
    "\n",
    "2. Найти собственные числа (спектр матрицы) и соответствующие им собственные числа матрицы C, решив характеристическое уравнение:\n",
    "\n",
    "![](data/9.PNG)\n",
    "\n",
    "3. Выбрать наибольшее собственное число из полученных λ = max(λ1,λ2) и соответствующий ему собственный вектор v.\n",
    "4. Координаты выбранного собственного вектора v1 и v2 будут являться коэффициентами линейной комбинации — главной компонентой:\n",
    "\n",
    "![](data/10.PNG)\n",
    "\n",
    "Примечание. Дополнительно делается **нормировка** нового признака. Её можно выполнить в самом конце шага 4 после пересчёта, а можно нормировать собственные векторы на шаге 3 (так делает компьютер). Наличие или отсутствие нормировки не повлияет на большинство свойств главных компонент."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## **РЕШЕНИЕ ЗАДАЧИ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍️ Давайте применим метод главных компонент для выбора оптимального признака в нашей задаче. Пусть NOX будет первым фактором, а DIS — вторым.\n",
    "\n",
    "1. Вычислим корреляционную матрицу C:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/e05ed4a6ee6ef6ee4e08fbc02e738d9f/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst-math-ml-3-17.jpg)\n",
    "\n",
    "Свойства корреляционной матрицы гарантируют нам положительные собственные числа и полный набор собственных векторов.\n",
    "\n",
    "2. Вычислим собственные числа и собственные векторы матрицы C:\n",
    "\n",
    "Собственные числа:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/107586cfd56e8599fb87517787a0c173/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst-math-ml-3-18.png)\n",
    "\n",
    "Собственные векторы:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/bbb7dd46518e784ebd5a8ba7adb5c55b/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst-math-ml-3-19.png)\n",
    "\n",
    "3. Выбираем максимальное из собственных чисел ![](data/11.PNG) и соответствующий ему собственный вектор, это будет ![](data/12.PNG)\n",
    "\n",
    "4. Координаты собственного вектора v будут коэффициентами для линейной комбинации старых признаков:\n",
    "\n",
    "![](data/13.PNG)\n",
    "\n",
    "Важно! При составлении нового фактора нужно брать именно стандартизированные признаки: центрированные и нормированные к единичной длине:\n",
    "\n",
    "![](data/14.PNG)\n",
    "\n",
    "5. Далее необходимо будет нормировать полученный фактор:\n",
    "\n",
    "![](data/15.PNG)\n",
    "\n",
    "На самом деле это даже не нормировка, а целая **стандартизация**, просто среднее значение NEWFACTOR будет нулевым по особенностям построения.\n",
    "\n",
    "NEWFACTORst называется главной компонентой. Есть ещё одна главная компонента, соответствующая меньшему собственному числу λ2 и собственному вектору v2. Для понижения размерности и борьбы с мультиколлинеарностью мы берём только первую главную компоненту.\n",
    "***\n",
    "Возникает вопрос: изменится ли алгоритм метода главных компонент в общем случае при k факторах?\n",
    "\n",
    "* Шаги 1 и 2 не изменятся.\n",
    "* Шаг 3. Для избавления от мультиколлинеарности нам нужно будет взять несколько самых больших собственных чисел и их айгенпар, а маленькие — игнорировать.\n",
    "* Шаг 4. Можно пересчитать старые признаки в новые матричными преобразованием."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **АНАЛИЗ АЛГОРИТМА**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Так как собственные векторы корреляционной матрицы ортогональны, то и новые признаки (главные компоненты) тоже будут ортогональны. Для нас это будет означать **нескоррелированность**.\n",
    "\n",
    "2. В случае плохой обусловленности выбор значимых главных компонент позволяет достичь меньшей потери точности по сравнению с регрессией на сырые данные.\n",
    "\n",
    "3. Можно искать не все собственные векторы, а только значимые — с «большими» собственными числами.\n",
    "\n",
    "4. Есть несколько подходов к тому, какие собственные числа считать «большими».\n",
    "\n",
    "Самый простой из них — **метод Кайзера**: значимыми считаются только те компоненты, у которых собственное число больше среднего значения всех собственных чисел λmean:\n",
    "\n",
    "![](data/16.PNG)\n",
    "\n",
    "Примечание. В нашем примере и согласно правилу Кайзера нужно выбрать одну компоненту.\n",
    "\n",
    "Есть также **метод сломанной трости**. Иногда просто берут несколько максимальных собственных значений.\n",
    "\n",
    "Важное замечание. В процессе выбора новых признаков никак не участвует целевая переменная. Если в нашем примере взять в качестве целевой переменной незначимую главную компоненту NOXst + DISst ,то наш новый классный признак вообще никак её не объяснит, потому что они ортогональны.\n",
    "\n",
    "*Тем не менее для БОЛЬШИНСТВА целевых переменных выбор значимых главных компонент даёт лучший прогноз среди всех возможных линейных комбинаций признаков.*\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ГРАНИЦЫ ПРИМЕНИМОСТИ**\n",
    "\n",
    "1. В случае хорошо обусловленных данных обрезать главные компоненты часто не нужно, так как потери точности могут оказаться больше, чем мы хотели бы.\n",
    "\n",
    "2. Для плохо обусловленных данных МАЛЫХ размерностей (для малого количества факторов) PCA — «то, что доктор прописал».\n",
    "\n",
    "3. Для плохо обусловленных данных БОЛЬШИХ размерностей (для большого количества факторов) при попытке применить PCA «в лоб» возникают вычислительные сложности.\n",
    "\n",
    "   - Дело в том, что если у нас, например, 10 000 факторов, то придётся считать, хранить и обрабатывать матрицу корреляций размера 10 000 × 10 000, что не слишком удобно с точки зрения памяти и скорости.\n",
    "\n",
    "   - Поэтому в таких случаях значимые главные компоненты вычисляют через сингулярное разложение матрицы данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.9922 0.0078] \n",
      " [[ 0.70710678 -0.70710678]\n",
      " [ 0.70710678  0.70710678]] \n",
      "\n",
      "[0.70710678 0.70710678]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.244 ,  0.8643, -0.2881, -0.3323])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.array([[1,0.9922],\n",
    "              [0.9922, 1]])\n",
    "\n",
    "x_1 = np.array([1,2,1,1])\n",
    "x_2 = np.array([70, 130, 65, 60])\n",
    "x_1_std = (x_1 - x_1.mean())/np.linalg.norm(x_1 - x_1.mean())\n",
    "x_2_std = (x_2 - x_2.mean())/np.linalg.norm(x_2 - x_2.mean())\n",
    "_, eig_vec = np.linalg.eig(A)\n",
    "print(_, '\\n', eig_vec, '\\n')\n",
    "eig_vec = eig_vec[1]\n",
    "print(eig_vec)\n",
    "x_new = eig_vec[0]*x_1_std + eig_vec[1]*x_2_std\n",
    "x_new_std = x_new / np.linalg.norm(x_new)\n",
    "x_new_std.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве дополнительной литературы рекомендуем вам:\n",
    "\n",
    "* ознакомиться с [**примером**](https://habr.com/ru/post/304214/) применения метода главных компонент;\n",
    "* прочесть [**статью**](http://www.machinelearning.ru/wiki/index.php?title=Метод_главных_компонент) о «Методе главных компонент»."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c68eacbc5a3a550d9b1f68bf11bf31e2b39ed4b9985227d4d8c7ee1d286013f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
