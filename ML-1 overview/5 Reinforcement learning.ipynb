{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Виды машинного обучения: обучение с подкреплением**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ Особенный вид машинного обучения — обучение с подкреплением.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/3ba2789cf42a41f284b23fc59417e155/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml1-5_1.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ОБУЧЕНИЕ С ПОДКРЕПЛЕНИЕМ**\n",
    "\n",
    "Обучение с подкреплением кардинально отличается от обучения с учителем и без него, поэтому его выделяют в отдельный вид обучения.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/60404dd3158cf2451c78c251e385ca77/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml1-5_2.png)\n",
    "\n",
    "**Это не задачи, связанные с анализом данных и предсказанием, а задачи взаимодействия со средой и «выживания» в ней.**\n",
    "\n",
    "Средой может быть видеоигра: в такой искусственной среде «выживают», например, роботы, играющие в Mario, или интеллектуальные боты во множестве других игр.\n",
    "\n",
    "Средой может быть и реальный мир, точнее — его часть: в такой среде существуют, например, автопилот Tesla, роботы-пылесосы или беспилотные летательные аппараты.\n",
    "\n",
    "**Объект, который взаимодействует со средой (например, играет в игру), называется агентом.**\n",
    "\n",
    "Агент может получать от среды полные или частичные наблюдения о её состоянии. Он может выполнять действия согласно своим наблюдениям. По мере совершения действий агент может получить в ответ награду от среды.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/dbe49cb82c3ab88dcfe6b2530da0f0fc/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml1-5_3.png)\n",
    "\n",
    "Например, в игре Mario действиями являются «прыгнуть», «сходить вперёд» или «присесть», а наградой — монетки, которые собирает персонаж.\n",
    "\n",
    "Данные о среде могут быть полезны агенту, но они не являются главным фактором обучения. Неважно, сколько данных соберёт агент, — у него всё равно не получится предусмотреть все возможные ситуации.\n",
    "\n",
    "**Поэтому цель обучения — не рассчитать все ходы, а построить оптимальную стратегию для взаимодействия со средой и максимизировать финальную награду.**\n",
    "\n",
    "**Выживание в среде — это и есть идея обучения с подкреплением. Давайте бросим бедного агента на растерзание судьбе и дадим ему неограниченное число жизней. Будем штрафовать его за ошибки и награждать за правильные поступки.**\n",
    "\n",
    "В математике уже давно известны методы, которые позволяют находить оптимальную стратегию. В основе обучения с подкреплением лежат теория игр, теория динамической оптимизации и ещё множество математических дисциплин.\n",
    "\n",
    "Например, классический метод обучения с подкреплением — **Q-learning** — основан на [уравнении Беллмана](https://habr.com/ru/post/443240/).\n",
    "\n",
    "В Q-learning рассматриваются все состояния, в которых может находиться агент, и все возможные переходы из одного состояния в другое, которые определяются действиями.\n",
    "\n",
    "Например, в игре Frozen Lake состояние определяется номером клетки на двумерном поле (на картинке их 16 штук). Наступив на некоторые клетки мы можем попасть в яму — это будет состояние проигрыша, а есть одна целевая клетка, попав в которую мы переходим в состояние выигрыша.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/1d334dc1ebd4126a8ddaf31e679e0f74/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst3-ml1-5_4.png)\n",
    "\n",
    "Вводится некоторая Q-функция, которая оценивает, какую награду получит агент при совершении действия из своего состояния.\n",
    "\n",
    "Уравнение Беллмана помогает определить следующее оптимальное действие, такое, что значение Q-функции для определённой пары состояние-действие будет максимальной.\n",
    "\n",
    "**Цель Q-learning — приближённо найти (аппроксимировать) Q-функцию, которая ответит на вопрос, как нужно правильно играть, чтобы получить максимум награды.**\n",
    "\n",
    "Однако чем больше состояний, тем сложнее отыскать Q-функцию. Что если клеток на поле не 16, а 160? А если это не поле, разделённое на клетки, а целая игровая карта со множеством возможных ходов? Наконец, а если это улицы огромного города? Математически это будет означать экспоненциальное увеличение сложности поиска Q-функции.\n",
    "\n",
    "Для сложных задач — сложные решения. В таких случаях для поиска сложных Q-функций привлекаются нейронные сети, а такое обучение носит название **Deep Q-Network (DQN)**.\n",
    "\n",
    "На идеях Q-learning основаны и другие алгоритмы, например алгоритм SARSA. Подробнее об алгоритмах можно почитать [здесь](https://habr.com/ru/post/561746/).\n",
    "\n",
    "***\n",
    "\n",
    "Отдельный пласт в сфере обучения с подкреплением — **генетические алгоритмы**. Их идея отличается от идей Q-learning и состоит в следующем: мы бросаем множество агентов в среду и заставляем их идти к цели. Затем мы выбираем лучших из них — тех, кто прошёл дальше всех, скрещиваем, добавляем мутации и бросаем в среду ещё раз. Такие манипуляции мы проделываем огромное количество раз. В итоге по законам эволюции должно получиться разумное существо. Главный вопрос — сколько времени на это может понадобиться.\n",
    "\n",
    "Как выяснилось на практике, генетические алгоритмы значительно уступают в скорости обучения методам Q-learning и их производным, поэтому они используются всё реже.\n",
    "***\n",
    "✍ Мы разобрали особенности обучения с подкреплением. Очевидно, что оно не похоже на другие виды машинного обучения.\n",
    "\n",
    "Мы не будем детально рассматривать обучение с подкреплением в нашем курсе, так как область его применения, как вы сами понимаете, очень специфична, и вы сможете познакомиться с ней самостоятельно, если встретите её на практике.\n",
    "\n",
    "Однако важно понимать ключевые моменты в обучении с подкреплением, чтобы, если столкнётесь с описанием задачи, вы могли бы сразу определить её тип и правильно начать поиски решения."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
