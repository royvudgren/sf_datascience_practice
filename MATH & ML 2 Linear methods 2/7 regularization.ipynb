{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Регуляризация**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ В предыдущем юните мы говорили о том, что полиномиальная регрессия склонна к переобучению. Это связано со сложностью модели и её способностью подстраиваться под очень сложные зависимости, из-за которых возникает высокий разброс.\n",
    "\n",
    "Рассмотрим пример ↓\n",
    "\n",
    "Обучим модель полиномиальной регрессии третьей степени. Будем использовать данные о жилье в Бостоне и возьмём следующие четыре признака: LSTAT, CRIM, PTRATIO и RM.\n",
    "\n",
    "Для оценки качества модели будем использовать кросс-валидацию и сравнивать среднее значение метрики на тренировочных и валидационных фолдах. Кросс-валидацию организуем с помощью функции **cross_validate** из модуля **model_selection**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Roman\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model\n",
    "import numpy as np # для работы с массивами\n",
    "import pandas as pd # для работы с DataFrame \n",
    "from sklearn import datasets # для импорта данных\n",
    "\n",
    "# загружаем датасет\n",
    "boston = datasets.load_boston()\n",
    "boston_data = pd.DataFrame(\n",
    "    data=boston.data, #данные\n",
    "    columns=boston.feature_names #наименования столбцов\n",
    ")\n",
    "boston_data['PRICE'] = boston.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метрики используем среднюю абсолютную процентную ошибку — MAPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE на тренировочных фолдах: 12.64 %\n",
      "MAPE на валидационных фолдах: 24.16 %\n"
     ]
    }
   ],
   "source": [
    "# выделяем интересующие нас факторы\n",
    "X = boston_data[['LSTAT', 'PTRATIO', 'RM','CRIM']]\n",
    "y = boston_data[['PRICE']]\n",
    " \n",
    "# добавляем полиномиальные признаки\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X = poly.fit_transform(X)\n",
    " \n",
    "# создаём модель линейной регрессии\n",
    "lr = linear_model.LinearRegression()\n",
    " \n",
    "# оцениваем качество модели на кросс-валидации, метрика — MAPE\n",
    "cv_results = cross_validate(lr, X, y, scoring='neg_mean_absolute_percentage_error', cv=5, return_train_score=True)\n",
    "print('MAPE на тренировочных фолдах: {:.2f} %'.format(-cv_results['train_score'].mean()* 100))\n",
    "print('MAPE на валидационных фолдах: {:.2f} %'.format(-cv_results['test_score'].mean() * 100))\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что мы видим? Даже при, казалось бы, небольшой, третьей степени полинома мы получили переобучение: на тренировочной выборке 12, а вот на тестовой — 24. Показатели качества отличаются практически в два раза, что говорит о высоком разбросе модели. Ещё более удручающий результат мы получим, если воспользуемся полиномом большей степени (при желании вы можете проверить это самостоятельно).\n",
    "\n",
    "Как с этим справиться, мы тоже уже знаем.\n",
    "\n",
    "* Можно попробовать понизить сложность модели (**снизить степень полинома**). Но до какой степени? Можно постепенно перебирать степень полинома до тех пор, пока не получим адекватные результаты, но, согласитесь, процедура не очень приятная.\n",
    "* Можно воспользоваться **методами регуляризации**.\n",
    "\n",
    "О втором способе как раз и поговорим подробнее с математической точки зрения.\n",
    "\n",
    "Для начала вспомним, что такое регуляризация.\n",
    "\n",
    "***\n",
    "**Регуляризация** — это способ уменьшения переобучения моделей машинного обучения путём намеренного **увеличения смещения** модели **для уменьшения её разброса**.\n",
    "***\n",
    "\n",
    "Регуляризация для линейной регрессии преследует сразу несколько целей. Однако далее мы увидим, что все эти цели на самом деле взаимосвязаны:\n",
    "\n",
    "* предотвратить переобучение модели;\n",
    "* включить в функцию потерь штраф за переобучение;\n",
    "* обеспечить существование обратной матрицы (A.T@A)^-1;\n",
    "* не допустить огромных коэффициентов модели.\n",
    "\n",
    "Мы знаем, что большие значения весов — прямое свидетельство переобучения модели линейной регрессии и её нестабильности. Идея регуляризации состоит в наложении ограничения на вектор весов (часто говорят — наложение штрафа за высокие веса). В качестве штрафа принято использовать **норму вектора весов**.\n",
    "\n",
    "Давайте запишем это на языке линейной алгебры. Вот задача минимизации длины вектора ошибок, о которой мы говорили, когда выводили формулу МНК:\n",
    "\n",
    "![](data/71.PNG)\n",
    "\n",
    "*Примечание. Обратите внимание на сумму под знаком корня. У нас она начинается с i = 0. Однако иногда в литературе, например [здесь](https://dyakonov.org/2019/10/31/линейная-регрессия/), можно встретить i = 1, то есть свободный член w0 рекомендуется не регуляризировать (не ограничивать). На самом деле это утверждение эвристическое и может как выполняться, так и нет, в зависимости от особенностей реализации. Мы будем придерживаться реализации в sklearn, в которой **w0 всё-таки включается в регуляризацию**.* \n",
    "\n",
    "Порядок нормы ***p*** в общем случае может быть любой — главное, чтобы она была больше 1. Однако на практике распространены только первая и вторая степени, так называемые **L1- и L2-регуляризации**. О них мы поговорим ниже, а пока доведём решение задачи до конца и получим общую формулу.\n",
    "\n",
    "Поставленная задача оптимизации называется **условной** — мы ищем минимум при некотором условии. Мы ещё не умеем решать такие задачи, но обязательно научимся, а пока на минутку заглянем в теорию оптимизации и поговорим о **методе множителей Лагранжа**. Прелесть данного метода в том, что он позволяет свести условную задачу оптимизации к безусловной, то есть благодаря Лагранжу мы можем перейти от системы к одному уравнению.\n",
    "***\n",
    "Метод множителей Лагранжа говорит, что записанная система с ограничением эквивалентна следующей записи:\n",
    "\n",
    "![](data/72.PNG)\n",
    "\n",
    "где L(w,a) — функция Лагранжа, которая зависит не только от вектора весов модели w, но и от некоторой константы a>=0 — множителя Лагранжа.\n",
    "\n",
    "Это и есть финальный результат, на котором мы пока что остановимся. По сути, ничего особо не изменилось по сравнению с изначальной задачей оптимизации. Добавилось только одно слагаемое — ![](data/73.PNG). Заметим, что если a=0, то мы получаем исходную задачу ![](data/74.PNG). Далее мы увидим, что это маленькое слагаемое очень сильно поможет нам победить переобучение модели.\n",
    "\n",
    "В машинном обучении множитель Лагранжа  принято называть **коэффициентом регуляризации**. Он отвечает за «силу» регуляризации. Чем он больше, тем меньшие значения может принимать слагаемое ![](data/75.PNG), то есть тем сильнее ограничения на норму весов. В этом и была наша цель — ограничить веса.\n",
    "\n",
    "Теперь разберёмся с **частными случаями**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## **L2-РЕГУЛЯРИЗАЦИЯ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнём мы, как ни странно, с L2-регуляризации, так как она очень наглядно показывает, как регуляризация обеспечивает невырожденность матрицы A.T@A.\n",
    "***\n",
    "* **L2-регуляризация (Ridge), или регуляризация по Тихонову** — это регуляризация, в которой порядок нормы p = 2. \n",
    "***\n",
    "Тогда, если подставить p = 2 в наши формулы, то оптимизационная задача в случае L2-регуляризации будет иметь вид:\n",
    "\n",
    "![](data/76.PNG)\n",
    "\n",
    "Примечание. Видно, что норма порядка p = 2 на самом деле является знакомой нам длиной вектора. То есть в случае L2-регуляризации мы накладываем ограничение на длину вектора весов w.\n",
    "\n",
    "В терминах функции Лагранжа задача будет выглядеть как:\n",
    "\n",
    "![](data/77.PNG)\n",
    "\n",
    "Примечание. В других реализациях аналитического решения регуляризации Тихонова, отличных от sklearn, где коэффициент w0 не участвует в регуляризации, единичная матрица E заменяется на матрицу I, в которой первый столбец и первая строка — нулевые. Это делается для того, чтобы исключить коэффициент w0 из регуляризации:\n",
    "\n",
    "![](data/78.PNG)\n",
    "\n",
    "Что мы в итоге получаем? Преимущество этой формулы в том, что, если a>0, то матрица A.T@A + aE гарантированно является невырожденной, даже если матрица A.T@A таковой не является. Так получается за счёт того, что по диагонали матрицы A.T@A мы добавляем поправки, которые создают линейную независимость между столбцами матрицы.\n",
    "\n",
    "Продемонстрируем это на примере ↓"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Пример № 1**\n",
    "\n",
    "Построить линейную регрессию с L2-регуляризацией, если:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/1177f21b44348a02e959c76c94516ee7/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/MATHML_md2_7_5.png)\n",
    "\n",
    "Коэффициент регуляризации a = 5.\n",
    "\n",
    "Давайте составим матрицу наблюдений A для нашей задачи:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/42ca4fda79a56c1f85f90c77f0fd16ce/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/MATHML_md2_7_6.png)\n",
    "\n",
    "Найдём матрицу Грама A.T@A: \n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/12c5a0f21c020a58afe20e8ac9782386/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/MATHML_md2_7_7.png)\n",
    "\n",
    "Очевидно, что матрица A.T@A вырождена: её второй и третий столбцы являются пропорциональными с коэффициентом 2. Значит, наша классическая формула МНК (без сингулярного разложения) не сработает.\n",
    "\n",
    "![](data/79.PNG)\n",
    "\n",
    "Проверим:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [12], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m4\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m4\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m7\u001b[39m])\n\u001b[0;32m      9\u001b[0m \u001b[39m# получаем оценку коэффициентов регрессии по МНК\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m w_hat \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49minv(A\u001b[39m.\u001b[39;49mT\u001b[39m@A\u001b[39;49m)\u001b[39m@A\u001b[39m\u001b[39m.\u001b[39mT\u001b[39m@y\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(w_hat)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36minv\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Roman\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\linalg\\linalg.py:552\u001b[0m, in \u001b[0;36minv\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m    550\u001b[0m signature \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mD->D\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m isComplexType(t) \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39md->d\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    551\u001b[0m extobj \u001b[39m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[1;32m--> 552\u001b[0m ainv \u001b[39m=\u001b[39m _umath_linalg\u001b[39m.\u001b[39;49minv(a, signature\u001b[39m=\u001b[39;49msignature, extobj\u001b[39m=\u001b[39;49mextobj)\n\u001b[0;32m    553\u001b[0m \u001b[39mreturn\u001b[39;00m wrap(ainv\u001b[39m.\u001b[39mastype(result_t, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m))\n",
      "File \u001b[1;32mc:\\Users\\Roman\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\linalg\\linalg.py:89\u001b[0m, in \u001b[0;36m_raise_linalgerror_singular\u001b[1;34m(err, flag)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_raise_linalgerror_singular\u001b[39m(err, flag):\n\u001b[1;32m---> 89\u001b[0m     \u001b[39mraise\u001b[39;00m LinAlgError(\u001b[39m\"\u001b[39m\u001b[39mSingular matrix\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "# матрица наблюдений (включая столбец единиц)\n",
    "A = np.array([\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [1, 0, -3, 2, 4],\n",
    "    [2, 0, -6, 4, 8]\n",
    "]).T\n",
    "# вектор целевого признака\n",
    "y = np.array([4, 3, -4, 2, 7])\n",
    "# получаем оценку коэффициентов регрессии по МНК\n",
    "w_hat = np.linalg.inv(A.T@A)@A.T@y\n",
    "print(w_hat) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы ожидаемо получили ошибку, говорящую о том, что матрица A вырождена. \n",
    "\n",
    "Теперь попробуем воспользоваться регуляризацией Тихонова. Для этого составляем матрицу E. Она будет размером 3x3 (количество параметров — 3):\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/3270bf11f0dcaf6b11a50376d7d6fc21/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/MATHML_md2_7_8.png)\n",
    "\n",
    "Добавим регуляризационное слагаемое к матрице A.T@A:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/7ad2d546088b44fdc84f8eeded7a402a/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/MATHML_md2_7_9.png)\n",
    "\n",
    "Видно, что матрица A.T@A+aE уже не будет вырожденной: её столбцы уже не являются линейно зависимыми, а значит решение будет существовать.\n",
    "\n",
    "Попробуем найти вектор оценок весов w_ridge по формуле:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6122449  0.29387755 0.5877551 ]\n"
     ]
    }
   ],
   "source": [
    "# матрица наблюдений (включая столбец единиц)\n",
    "A = np.array([\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [1, 0, -3, 2, 4],\n",
    "    [2, 0, -6, 4, 8]\n",
    "]).T\n",
    "# вектор целевого признака\n",
    "y = np.array([4, 3, -4, 2, 7])\n",
    "# единичная матрица\n",
    "E = np.eye(3)\n",
    "# коэффициент регуляризации \n",
    "alpha = 5\n",
    "# получаем оценку коэффициентов регрессии по МНК с регуляризацией Тихонова\n",
    "w_hat_ridge = np.linalg.inv(A.T@A+alpha*E)@A.T@y\n",
    "print(w_hat_ridge) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работает! Мы получили вектор весов.\n",
    "\n",
    "Итак, мы посмотрели, как работает аналитическое решение L2-регуляризации. Однако в реализации sklearn для решения этой задачи поддерживается сразу несколько методов — как численных (координатный спуск, градиентный спуск или [LBFGS](https://ru.wikipedia.org/wiki/Алгоритм_Бройдена_—_Флетчера_—_Гольдфарба_—_Шанно)), так и аналитических (классическая регуляризация Тихонова или она же через SVD-разложение). По умолчанию метод выбирается автоматически. На простых данных все методы будут показывать примерно одинаковые результаты при одном и том же значении коэффициента регуляризации, однако на реальных данных, когда данные не стандартизированы и присутствует сильная мультиколлинеарность между факторами, результат работы каждого из методов решения задачи оптимизации может значительно отличаться. Имейте это в виду при построении модели. Подробнее о методах вы можете прочитать в документации.\n",
    "\n",
    "Напомним, что за реализацию линейной регрессии в sklearn отвечает класс [**Ridge**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html). Основной параметр модели, на который стоит обратить внимание — **alpha**, коэффициент регуляризации из формулы Тихонова.\n",
    "\n",
    "Давайте обучим модель для решения нашей последней задачи, а затем проверим коэффициенты регрессии. Так как мы заранее заложили в матрицу A столбец из единиц, то, чтобы получить корректное решение, параметр **fit_intercept** следует установить в значение False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6122449  0.29387755 0.5877551 ]\n"
     ]
    }
   ],
   "source": [
    "# матрица наблюдений (включая столбец единиц)\n",
    "A = np.array([\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [1, 0, -3, 2, 4],\n",
    "    [2, 0, -6, 4, 8]\n",
    "]).T\n",
    "# вектор целевого признака\n",
    "y = np.array([4, 3, -4, 2, 7])\n",
    "# получаем оценку коэффициентов регрессии по МНК с регуляризацией Тихонова\n",
    "ridge = Ridge(alpha=5, fit_intercept=False)\n",
    "ridge.fit(A, y)\n",
    "print(ridge.coef_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили тот же самый результат, что и раньше.\n",
    "***\n",
    "Наконец, посмотрим, как регуляризация поможет побороть переобучение модели полиномиальной регрессии на наборе данных о домах в Бостоне. Используем те же самые признаки: LSTAT, CRIM, PTRATIO и RM. \n",
    "\n",
    "Сразу отметим, что для успешной сходимости численных методов оптимизации, которые используются для решения задачи условной оптимизации, необходима **стандартизация** (нормализация) исходных данных, которая не требовалась для аналитического МНК в классической линейной регрессии (LinearRegression).\n",
    "\n",
    "*Примечание. Здесь под стандартизацией мы понимаем именно приведение распределения признака к нулевому среднему и единичному стандартному отклонению (StandartScaler), а не стандартизацию векторов, о которой мы говорили в этом модуле. Последнюю также можно использовать в качестве способа масштабирования данных, однако её реализации нет в sklearn.*\n",
    "\n",
    "Воспользуемся моделью полиномиальной регрессии третьей степени с регуляризацией Тихонова (коэффициент регуляризации возьмём равным 20) и проверим её качество на кросс-валидации по метрике MAPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE на тренировочных фолдах: 12.54 %\n",
      "MAPE на валидационных фолдах: 17.02 %\n"
     ]
    }
   ],
   "source": [
    "# выделяем интересующие нас факторы\n",
    "X = boston_data[['LSTAT', 'PTRATIO', 'RM','CRIM']]\n",
    "y = boston_data[['PRICE']]\n",
    "# инициализируем стандартизатор StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# подгоняем параметры стандартизатора (вычисляем среднее и СКО)\n",
    "X = scaler.fit_transform(X)\n",
    "# добавляем полиномиальные признаки\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X = poly.fit_transform(X)\n",
    "# создаём модель линейной регрессии c L2-регуляризацией\n",
    "ridge = Ridge(alpha=20, solver='svd')\n",
    "# оцениваем качество модели на кросс-валидации\n",
    "cv_results = cross_validate(ridge, X, y, scoring='neg_mean_absolute_percentage_error', cv=5, return_train_score=True)\n",
    "print('MAPE на тренировочных фолдах: {:.2f} %'.format(-cv_results['train_score'].mean()* 100))\n",
    "print('MAPE на валидационных фолдах: {:.2f} %'.format(-cv_results['test_score'].mean() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам удалось уменьшить ошибку (MAPE) на валидационных фолдах кросс-валидации с 24.16% до 17.02% и сократить разницу в метриках, тем самым уменьшив разброс ответов модели.\n",
    "\n",
    "Теперь перейдём к L1-регуляризации.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ЗАДАЧА**\n",
    "\n",
    "Вычислите коэффициенты линейной регрессии с L2-регуляризацией, используя аналитическую формулу Тихонова, если:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/26955960b6372607132b2a005abafbc9/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/MATHML_md2_7_10.png)\n",
    "\n",
    "Коэффициент регуляризации a=1.\n",
    "\n",
    "В качестве ответа приведите значения полученных коэффициентов линейной регрессии, округлив их до второго знака после точки-разделителя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.09 -1.71  1.91  0.73]\n"
     ]
    }
   ],
   "source": [
    "# матрица наблюдений (включая столбец единиц)\n",
    "A = np.array([\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [5, 9, 4, 3, 5],\n",
    "    [15, 18, 18, 19, 19],\n",
    "    [7, 6, 7, 7, 7]\n",
    "]).T\n",
    "# вектор целевого признака\n",
    "y = np.array([24, 22, 35, 33, 36])\n",
    "alpha = 1\n",
    "E = np.eye(4)\n",
    "# получаем оценку коэффициентов регрессии по МНК с регуляризацией Тихонова\n",
    "w_hat_ridge = np.linalg.inv(A.T@A+alpha*E)@A.T@y\n",
    "print(w_hat_ridge.round(2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## **L1-РЕГУЛЯРИЗАЦИЯ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **L1-регуляризацией, Lasso** (Least Absolute Shrinkage and Selection Operator), называется регуляризация, в которой порядок нормы p = 1.\n",
    "***\n",
    "\n",
    "Тогда оптимизационная задача в случае L1-регуляризации будет иметь вид:\n",
    "\n",
    "![](data/80.PNG)\n",
    "\n",
    "Примечание. Таким образом, в случае L1-регуляризации мы ограничиваем **сумму модулей весов модели**. Такая величина называется **[нормой Манхэттена](https://ru.wikipedia.org/wiki/Расстояние_городских_кварталов) (расстоянием городских кварталов)**.\n",
    "\n",
    "Запишем полученную систему в терминах метода Лагранжа:\n",
    "\n",
    "![](data/81.PNG)\n",
    "\n",
    "Можно показать, что данная задача имеет аналитическое решение, однако в реализации sklearn оно даже не заявлено как возможное для использования в связи с нестабильностью взятия производной от функции модуля, поэтому мы не будем его рассматривать. Ознакомиться с ним вы можете [здесь](http://www.machinelearning.ru/wiki/images/7/7e/VetrovSem11_LARS.pdf).\n",
    "\n",
    "В sklearn L1-регуляризация реализована в классе **Lasso**, а заданная выше оптимизационная задача решается алгоритмом **координатного спуска (Coordinate Descent)**.\n",
    "\n",
    "Давайте посмотрим, как работает Lasso на «игрушечном» примере, а затем применим его для набора данных о домах в Бостоне."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Пример № 2**\n",
    "\n",
    "Построить линейную регрессию с L1-регуляризацией, если:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/d6b4f37e6250dd672fe066b8642ee778/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/MATHML_md2_7_13.png)\n",
    "\n",
    "Коэффициент регуляризации a=0.1.\n",
    "\n",
    "Составим матрицу наблюдений A для нашей задачи:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/e054cc4c29a1b089b542f008c21ff2b1/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/MATHML_md2_7_14.png)\n",
    "\n",
    "Из примера №1 мы уже знаем, что матрица Грама A.T@A будет вырождена, а значит классического МНК-решения (не беря в расчёт сингулярное разложение) не получится.\n",
    "\n",
    "Попробуем найти коэффициенты регрессии с помощью L1-регуляризации. Для этого подадим нашу матрицу наблюдений A и вектор целевого признака y в модель Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.14925373 0.         0.71921642]\n"
     ]
    }
   ],
   "source": [
    "# матрица наблюдений (включая столбец единиц)\n",
    "A = np.array([\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [1, 0, -3, 2, 4],\n",
    "    [2, 0, -6, 4, 8]\n",
    "]).T\n",
    "# вектор целевого признака\n",
    "y = np.array([4, 3, -4, 2, 7])\n",
    "# получаем оценку коэффициентов регрессии с помощью L1-регуляризации\n",
    "lasso = Lasso(alpha=0.1, fit_intercept=False)\n",
    "lasso.fit(A, y)\n",
    "print(lasso.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот наша оценка вектора весов:\n",
    "\n",
    "![](data/82.PNG)\n",
    "\n",
    "Сразу обращаем внимание, что, в отличие от регуляризации Тихонова, L1-регуляризация «занулила» коэффициент, стоящий при факторе x1. Это произошло не случайно, так как это особенность данного метода. Как говорится, «не баг, а фича», причём очень важная. Коэффициенты, стоящие при коллинеарных или высококоррелированных факторах, зануляются. Также чем выше коэффициент регуляризации, тем больше вероятность того, что коррелированные или малозначащие факторы будут исключены из модели. Чуть позже мы рассмотрим геометрическую интерпретацию и поймём, почему так происходит.\n",
    "***\n",
    "А пока давайте применим L1-регуляризацию к нашей полиномиальной модели третьей степени, прогнозирующей типичную цену на дома в районах Бостона.\n",
    "\n",
    "Так как метод координатного спуска, который применяется для поиска коэффициентов, является численным, то необходима стандартизация исходных данных, чтобы обеспечить ему сходимость. Возьмём в качестве коэффициента регуляризации a = 0.01 и проверим качество полученной модели с помощью кросс-валидации по метрике MAPE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE на тренировочных фолдах: 12.44 %\n",
      "MAPE на валидационных фолдах: 16.44 %\n"
     ]
    }
   ],
   "source": [
    "# выделяем интересующие нас факторы\n",
    "X = boston_data[['LSTAT', 'PTRATIO', 'RM','CRIM']]\n",
    "y = boston_data[['PRICE']]\n",
    "\n",
    "# инициализируем стандартизатор StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# подгоняем параметры стандартизатора (вычисляем среднее и СКО)\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# добавляем полиномиальные признаки\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X = poly.fit_transform(X)\n",
    "\n",
    "# создаём модель линейной регрессии c L1-регуляризацией\n",
    "lasso = Lasso(alpha=0.1, max_iter=10000)\n",
    "\n",
    "# оцениваем качество модели на кросс-валидации\n",
    "cv_results = cross_validate(lasso, X, y, scoring='neg_mean_absolute_percentage_error', cv=5, return_train_score=True)\n",
    "print('MAPE на тренировочных фолдах: {:.2f} %'.format(-cv_results['train_score'].mean()* 100))\n",
    "print('MAPE на валидационных фолдах: {:.2f} %'.format(-cv_results['test_score'].mean() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что с помощью L1-регуляризации удалось уменьшить ошибку модели (MAPE) на валидационных фолдах с 24.16% до 16.44% и сократить разницу в метриках на тренировочных и валидационных фолдах даже лучше, чем с этим справилась L2-регуляризация. Однако на самом деле мы просто удачно выбрали коэффициент регуляризации — при других значениях могли получиться совершенно другие результаты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## **ELASTIC-NET**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последний вид регуляризации (хотя их на самом деле больше), который мы рассмотрим, называется **Elastic-Net (эластичная сетка)**. Это комбинация L1- и L2-регуляризации.\n",
    "\n",
    "Идея Elastic-Net состоит в том, что мы вводим ограничение как на норму весов порядка p = 1, так и на норму порядка p = 2. Тогда оптимизационная задача будет иметь вид:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/7399b8f59ea951786a5825a891118f3f/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/MATHML_md2_7_15.png)\n",
    "\n",
    "Немного модифицировав формулу функции Лагранжа, которая получается в результате такой задачи условной оптимизации, можно получить финальный результат:\n",
    "\n",
    "![](data/83.PNG)\n",
    "\n",
    "Аналитического решения у этой задачи нет, поэтому для её решения в sklearn, как и для модели Lasso, используется **координатный спуск**.\n",
    "\n",
    "В sklearn эластичная сетка реализована в классе **ElasticNet** из пакета с линейными моделями — **linear_model**. За коэффициент alpha отвечает параметр *alpha*, за коэффициент lambda — *l1_ratio*.\n",
    "\n",
    "Некоторые рекомендации от разработчиков ElasticNet:\n",
    "\n",
    "* Использование параметра **l1_ratio** <0.01 приводит к нестабильным результатам.\n",
    "* Вместо использования ElasticNet с **alpha**=0 лучше используйте **LinearRegression**, так как там применяется аналитическое решение, которое позволяет получать более точные решения, чем численный координатный спуск.\n",
    "\n",
    "По традиции рассмотрим «игрушечный» пример работы с Elastic-Net, а затем применим эту модель к нашей задаче о домах в Бостоне."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Пример № 3**\n",
    "\n",
    "Построить линейную регрессию с Elastic-Net-регуляризацией, если:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/b5b24227906fb067bd68d190be99ee76/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/MATHML_md2_7_16.png)\n",
    "\n",
    "Решить задачу с тремя комбинациями коэффициентов регуляризации:\n",
    "\n",
    "![](data/84.PNG)\n",
    "\n",
    "Составим матрицу наблюдений A для нашей задачи:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/3cc67a89e412a9fe2ff9da81514c48b8/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/MATHML_md2_7_17.png)\n",
    "\n",
    "Мы уже знаем, что матрица Грама  будет вырождена, а значит классического МНК-решения (не беря в расчёт сингулярное разложение) не получится.\n",
    "\n",
    "Сразу переходим к построению регрессии с помощью ElasticNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.13492457 0.19525842 0.6237965 ]\n"
     ]
    }
   ],
   "source": [
    "# СЛУЧАЙ 1 a = 0.1 lambda = 0.2\n",
    "\n",
    "# матрица наблюдений (включая столбец единиц)\n",
    "A = np.array([\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [1, 0, -3, 2, 4],\n",
    "    [2, 0, -6, 4, 8]\n",
    "]).T\n",
    "# вектор целевого признака\n",
    "y = np.array([4, 3, -4, 2, 7])\n",
    "# получаем оценку коэффициентов регрессии \n",
    "lasso = ElasticNet(alpha=0.1, l1_ratio=0.2, fit_intercept=False)\n",
    "lasso.fit(A, y)\n",
    "print(lasso.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратим внимание, что зануления коэффициентов коллинеарных факторов x1 и x2 не произошло. Каждый из них вошёл в уравнение регрессии с ненулевым коэффициентом.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.14379753 0.         0.71993025]\n"
     ]
    }
   ],
   "source": [
    "# СЛУЧАЙ 2 a = 0.1 lambda = 0.7\n",
    "\n",
    "# получаем оценку коэффициентов регрессии\n",
    "lasso = ElasticNet(alpha=0.1, l1_ratio=0.7, fit_intercept=False)\n",
    "lasso.fit(A, y)\n",
    "print(lasso.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратим внимание, что произошло зануление коэффициентов. Это неспроста, так как мы понизили влияние L2-регуляризации и одновременно повысили влияние L1-регуляризации, которая, как мы уже знаем, приводит к исключению линейно зависимых факторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.14925373 0.         0.71921642]\n"
     ]
    }
   ],
   "source": [
    "# СЛУЧАЙ 3 a = 0.1 lambda = 1\n",
    "\n",
    "# получаем оценку коэффициентов регрессии\n",
    "lasso = ElasticNet(alpha=0.1, l1_ratio=1, fit_intercept=False)\n",
    "lasso.fit(A, y)\n",
    "print(lasso.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В округлениях значения не заметно, однако если присмотреться к коэффициентам более внимательно, можно увидеть, что мы получили в точности те же значения, которые получали для модели Lasso в примере № 2. Неудивительно, ведь мы обнулили влияние L2-регуляризации, выставив l1_ratio=1. По сути, мы использовали чистую модель Lasso.\n",
    "\n",
    "Возникает вопрос: какой набор коэффициентов линейной регрессии всё-таки подходит лучше?\n",
    "\n",
    "Ответить на него можно, только вычислив метрику качества и сравнив ошибки прогнозов каждой из полученных моделей. Мы уверены, вы можете сделать это самостоятельно.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам осталось только попробовать применить Elastic-Net к данным о недвижимости в Бостоне.\n",
    "\n",
    "Как и для других моделей с регуляризацией, для Elastic-Net также лучше заранее позаботиться о стандартизации данных. В качестве коэффициентов регуляризации возьмём a = 0.1, lambda = 0.5. Качество модели проверим с помощью кросс-валидации на пяти фолдах, метрика — MAPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE на тренировочных фолдах: 12.65 %\n",
      "MAPE на валидационных фолдах: 15.70 %\n"
     ]
    }
   ],
   "source": [
    "# выделяем интересующие нас факторы\n",
    "X = boston_data[['LSTAT', 'PTRATIO', 'RM','CRIM']]\n",
    "y = boston_data[['PRICE']]\n",
    "# инициализируем стандартизатор StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# подгоняем параметры стандартизатора (вычисляем среднее и СКО)\n",
    "X = scaler.fit_transform(X)\n",
    "# добавляем полиномиальные признаки\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X = poly.fit_transform(X)\n",
    "# создаём модель линейной регрессии c L1- и L2-регуляризациями\n",
    "lasso = ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=10000)\n",
    "# оцениваем качество модели на кросс-валидации\n",
    "cv_results = cross_validate(lasso, X, y, scoring='neg_mean_absolute_percentage_error', cv=5, return_train_score=True)\n",
    "print('MAPE на тренировочных фолдах: {:.2f} %'.format(-cv_results['train_score'].mean()* 100))\n",
    "print('MAPE на валидационных фолдах: {:.2f} %'.format(-cv_results['test_score'].mean() * 100)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, Elastic-Net позволил нам уменьшить значение MAPE на валидационных фолдах с 24.16% до 15.7%. Отличный результат! Он получился лучше, чем у моделей Ridge и Lasso, но опять же скажем, что так бывает не всегда.\n",
    "\n",
    "→ На практике при использовании моделей с регуляризацией стоит подбирать значения коэффициентов регуляризации с помощью методов подбора гиперпараметров, которые мы изучали в модуле «ML-7. Оптимизация гиперпараметров модели». Только после подбора гиперпараметров можно сделать вывод, какая из моделей показывает наилучшие результаты для решения конкретной задачи. Надеемся, вы помните, как подбираются гиперпараметры (если нет, освежите знания в модуле [ML-7](https://lms.skillfactory.ru/courses/course-v1:SkillFactory+DSPR-2.0+14JULY2021/jump_to_id/cfba32dd83a948ec811afc5132f079c5))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## ***ГЕОМЕТРИЧЕСКАЯ ИНТЕРПРЕТАЦИЯ РЕГУЛЯРИЗАЦИИ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напоследок поговорим о геометрической интерпретации регуляризации. В дальнейшем, изучая теорию оптимизации, мы увидим, что задача условной оптимизации\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/ff4221dc97f0998ee63d55765b87f02d/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/MATHML_md2_7_18.png)\n",
    "\n",
    "![](data/85.PNG)\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/d584cd8689e7909149674a3805917c48/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/MATHML_md2_7_19.png)\n",
    "\n",
    "Концентрическими кругами обозначены линии равного уровня функции L(w). Для каждого конкретного набора данных она будет иметь разный вид, но смысл будет тем же. Голубой областью обозначены ромб и окружность, которые задаёт L1- и L2-норма вектора весов соответственно.\n",
    "\n",
    "* Если бы мы использовали классическую линейную регрессию, то МНК приводил бы нас в точку истинного минимума функции L(w) — в центр, из которого исходят концентрические круги. Это была бы некоторая комбинация параметров w0 и w1.\n",
    "* В случае, когда мы используем модель линейной регрессии с регуляризацией, мы будем пытаться найти такую комбинацию w0 и w1, которая доставляет минимум функции L(w), но при этом не выходит за границы ромба (или окружности). Таким образом, вместо истинного минимума мы находим так называемый **псевдоминимум**.\n",
    "\n",
    "Заметим, что у ромба вероятность коснуться концентрического круга одной из своих вершин больше, чем у окружности — своей верхней/нижней/правой/левой точкой. Точка касания в вершине ромба — это точка, в которой либо w0 = 0 либо w1 = 0. То есть L1-регуляризация склонна с большей вероятностью занулять коэффициенты линейной регрессии, чем L2-регуляризация.\n",
    "\n",
    "Величина диагонали ромба и радиуса окружности зависят от величины коэффициента регуляризации ***a***: чем больше ***a***, тем меньше ромб/окружность, а значит тем дальше псевдоминимум будет находиться от истинного минимума, и наоборот."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c68eacbc5a3a550d9b1f68bf11bf31e2b39ed4b9985227d4d8c7ee1d286013f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
