{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Квазиньютоновские методы**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ В предыдущем юните мы рассмотрели метод Ньютона. В отличие от градиентного спуска, метод Ньютона использует на каждой итерации не только градиент, но и матрицу Гессе. Это обеспечивает более быструю сходимость к минимуму, но в то же время это приводит к слишком большим вычислительным затратам. В данном юните мы рассмотрим класс методов, в которых решается эта проблема, — класс **квазиньютоновских методов**.\n",
    "\n",
    "Напомним, что в методе Ньютона мы обновляем точку на каждой итерации в соответствии со следующим правилом:\n",
    "\n",
    "![](data/12.PNG)\n",
    "\n",
    "На каждом шаге здесь вычисляется гессиан, а также его обратная матрица.\n",
    "\n",
    "В квазиньютоновских методах вместо вычисления гессиана мы просто аппроксимируем его матрицей, которая обновляется от итерации к итерации с использованием информации, вычисленной на предыдущих шагах. Так как вместо вычисления большого количества новых величин мы используем найденные ранее значения, квазиньютоновский алгоритм тратит гораздо меньше времени и вычислительных ресурсов.\n",
    "\n",
    "Формально это описывается следующим образом:\n",
    "\n",
    "![](data/13.PNG)\n",
    "\n",
    "В данном случае вместо обратного гессиана появляется матрица Hk, которая строится таким образом, чтобы максимально точно аппроксимировать настоящий обратный гессиан.\n",
    "\n",
    "Математически это записывается так:\n",
    "\n",
    "![](data/14.PNG)\n",
    "\n",
    "Здесь имеется в виду, что разница между матрицей в квазиньютоновском методе и обратным гессианом стремится к нулю.\n",
    "\n",
    "Эта матрица обновляется на каждом шаге, и для этого существуют разные способы. Для каждого из способов есть своя модификация квазиньютоновского метода. Эти способы объединены ограничением: **процесс обновления матрицы** должен быть достаточно эффективным и **не должен требовать вычислений гессиана**. То есть, по сути, на каждом шаге мы должны получать информацию о гессиане, не находя непосредственно сам гессиан.\n",
    "\n",
    "Если вас интересует математическая сторона обновления и аппроксимации матрицы, прочитайте эту [**статью**](http://www.machinelearning.ru/wiki/images/6/65/MOMO17_Seminar6.pdf). В силу того, что понимание этой части метода требует очень серьёзной математической подготовки, мы опустим её. Однако можем заверить вас, что для успешного использования алгоритма и его понимания знание всех математических выводов не требуется.\n",
    "\n",
    "**Три самые популярные схемы аппроксимации:**\n",
    "\n",
    "* симметричная коррекция ранга 1 (SR1);\n",
    "* схема Дэвидона — Флетчера — Пауэлла (DFP);\n",
    "* схема Бройдена — Флетчера — Гольдфарба — Шанно (BFGS).\n",
    "\n",
    "Последняя схема (BFGS) самая известная, стабильная и считается наиболее эффективной. На ней мы и остановимся. Своё название она получила из первых букв фамилий создателей и исследователей данной схемы: Чарли Джорджа Бройдена, Роджера Флетчера, Дональда Гольдфарба и Дэвида Шанно.\n",
    "\n",
    "У этой схемы есть две известных вариации:\n",
    "\n",
    "* L-BFGS;\n",
    "* L-BFGS-B.\n",
    "\n",
    "Обе этих вариации необходимы в случае большого количества переменных для экономии памяти (так как во время их реализации хранится ограниченное количество информации). По сути, они работают одинаково, и L-BFGS-B является лишь улучшенной версией L-BFGS для работы с ограничениями.\n",
    "\n",
    "Метод BFGS очень устойчив и на данный момент считается одним из наиболее эффективных. Поэтому, если, например, применить функцию optimize без указания метода в библиотеке SciPy, то по умолчанию будет использоваться именно BFGS либо одна из его модификаций, указанных выше. Также данный метод используется в библиотеке sklearn при решении задачи логистической регрессии.\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим **алгоритм применения этого метода**. Постарайтесь понять последовательность действий и основной принцип.\n",
    "\n",
    "                                                    1\n",
    "Реализация алгоритма начинается с того, что мы задаём начальную точку x0, выбираем точность алгоритма, а также изначальную аппроксимацию для обратного гессиана функции. \n",
    "\n",
    "Здесь как раз таится главная проблема: нет никакого универсального рецепта для выбора этого приближения. Можно поступить по-разному: использовать гессиан, вычисленный в изначальной точке, взять единичную матрицу (такая настройка стоит по умолчанию в некоторых функциях библиотек Python) или другую матрицу, если она невырождена и хорошо обусловлена.\n",
    "\n",
    "                                                    2\n",
    "Когда мы определили, откуда будем начинать, необходимо понять, как попасть в следующую точку. Поэтому на втором шаге мы вычисляем направление для поиска следующей точки:\n",
    "\n",
    "![](data/15.PNG)\n",
    "\n",
    "                                                    3\n",
    "Далее находим следующую точку, используя соотношение:\n",
    "\n",
    "![](data/16.PNG)\n",
    "\n",
    "Здесь важным вопросом является нахождение коэффициента k, который регулирует шаг. Его подбирают линейным поиском в соответствии с условиями, о которых можно подробно прочитать [**здесь**](https://en.wikipedia.org/wiki/Wolfe_conditions).\n",
    "\n",
    "![](data/17.PNG)\n",
    "\n",
    "                                                    4\n",
    "На следующем шаге необходимо определить два следующих вектора:\n",
    "\n",
    "![](data/18.PNG)\n",
    "\n",
    "Здесь sk — шаг алгоритма, a yk — изменение градиента на данной итерации. Они не требовались для перехода в следующую точку и нужны строго для того, чтобы найти следующее приближение обратной матрицы гессиана.\n",
    "\n",
    "                                                    5\n",
    "После того как мы их нашли, обновляем гессиан, руководствуясь следующей формулой:\n",
    "\n",
    "![](data/19.PNG)\n",
    "\n",
    "Не стоит её пугаться. Как уже было сказано выше, эта формула —  результат очень серьёзных и длительных математических исследований. Её не требуется знать наизусть или уметь реализовывать вручную. Однако для полноты повествования мы не можем не привести её.\n",
    "\n",
    "В данной формуле за I обозначена единичная матрица, sk и yk мы вычислили на предыдущем шаге, а k вычисляется следующим образом:\n",
    "\n",
    "![](data/20.PNG)\n",
    "\n",
    "Алгоритм довольно сложный, поэтому давайте рассмотрим пример ↓\n",
    "\n",
    "Сразу оговоримся, что несколько шагов в этом алгоритме мы приведём без ручных расчётов (например, нахождение следующей аппроксимации гессиана) в силу их высокой сложности. Постарайтесь сильнее всего сконцентрироваться на шагах решения и понять логику работы алгоритма и последовательность действий. Мы начнём реализовывать алгоритм «вручную», а затем вы сможете самостоятельно завершить решение задачи с использованием Python (разумеется, далее будет указано, как это сделать). К сожалению, серьёзные и эффективные методы настолько сложны, что решать с их помощью задачи, используя только лист бумаги, ручку и калькулятор (как мы могли это делать, например, с методом Лагранжа), уже невозможно.\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ПРИМЕР**\n",
    "\n",
    "![](data/21.PNG)\n",
    "![](data/22.PNG)\n",
    "![](data/23.PNG)\n",
    "![](data/24.PNG)\n",
    "\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разумеется, при решении прикладных задач не понадобится делать ничего подобного, ведь мы умеем программировать и знаем, что в библиотеках Python есть все необходимые методы.\n",
    "\n",
    "Давайте рассмотрим, как с помощью функций Python мы сможем применить квазиньютоновские методы для оптимизации функции x** 2 + y**2.\n",
    "\n",
    "Подгрузим необходимые библиотеки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статус оптимизации Optimization terminated successfully.\n",
      "Количество оценок: 3\n",
      "Решение: f([0. 0.]) = 0.00000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Определим функцию, которую будем оптимизировать.\n",
    "# Вместо отдельных x и y можно взять координаты единого вектора:\n",
    "def func(x):\n",
    "    return x[0]**2.0 + x[1]**2.0\n",
    "\n",
    "# Теперь определим градиент для функции:\n",
    "def grad_func(x):\n",
    "    return np.array([x[0] * 2, x[1] * 2])\n",
    "\n",
    "# Зададим начальную точку:\n",
    "x_0 = [1.0, 1.0]\n",
    "\n",
    "# Определим алгоритм:\n",
    "result = minimize(func, x_0, method='BFGS', jac=grad_func)\n",
    "\n",
    "#Выведем результаты:\n",
    "print('Статус оптимизации %s' % result['message'])\n",
    "print('Количество оценок: %d' % result['nfev'])\n",
    "solution = result['x']\n",
    "evaluation = func(solution)\n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, мы получили, что минимум функции достигается в точке (0,0). Значение функции в этой точке также равно нулю.\n",
    "\n",
    "Можно повторить то же самое с вариацией L-BFGS-B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статус оптимизации CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
      "Количество оценок: 3\n",
      "Решение: f([0. 0.]) = 0.00000\n"
     ]
    }
   ],
   "source": [
    "# определяем нашу функцию\n",
    "def func(x):\n",
    "    return x[0]**2.0 + x[1]**2.0\n",
    " \n",
    "#  определяем градиент функции\n",
    "def grad_func(x):\n",
    "    return np.array([x[0] * 2, x[1] * 2])\n",
    " \n",
    "# определяем начальную точку\n",
    "x_0 = [1, 1]\n",
    "# реализуем алгоритм L-BFGS-B\n",
    "result = minimize(func, x_0, method='L-BFGS-B', jac=grad_func)\n",
    "# получаем результат\n",
    "print('Статус оптимизации %s' % result['message'])\n",
    "print('Количество оценок: %d' % result['nfev'])\n",
    "solution = result['x']\n",
    "evaluation = func(solution)\n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат будет тем же самым.\n",
    "\n",
    "→ Иногда количество итераций у двух модификаций различается, но ответ совпадает. Бывает также, что одна из вариаций может не сойтись, а другая — достичь экстремума, поэтому советуем не воспринимать их как взаимозаменяемые алгоритмы. На практике лучше пробовать разные варианты: если у вас не сошёлся алгоритм BFGS, можно попробовать L-BFGS-B, и наоборот. Также можно экспериментировать одновременно с обоими алгоритмами, чтобы выбрать тот, который будет сходиться для функции за меньшее число итераций и тем самым экономить время.\n",
    "\n",
    "→ Важно понимать, что для некоторых функций не из всех стартовых точек получается достичь сходимости метода. Тогда их можно перебирать, к примеру, с помощью цикла.\n",
    "\n",
    "✍ Итак, мы обсудили один из самых эффективных на сегодняшний день алгоритмов — вариацию BFGS квазиньютоновских методов. Вы будете регулярно сталкиваться с этим алгоритмом при решении различных задач и при использовании библиотек для оптимизации. Так что давайте попрактикуемся: в этом юните мы посмотрели фрагмент поэтапного разбора метода BFGS для функции x** 2 - x*y + y **2 + 9 *x - 6 *y + 20 — давайте завершим начатое и найдём точку минимума ↓"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ЗАДАЧА**\n",
    "\n",
    "Найдите точку минимума для функции x** 2 - x*y + y **2 + 9 *x - 6 *y + 20.\n",
    "\n",
    "В качестве стартовой возьмите точку (-400, -400).\n",
    "\n",
    "Значения координат округлите до целого числа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статус оптимизации CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
      "Количество оценок: 9\n",
      "Решение: f([-3.99999972  1.00000028]) = -1.00000\n"
     ]
    }
   ],
   "source": [
    "# определяем нашу функцию\n",
    "def func(x):\n",
    "    return x[0]**2.0 - x[0]*x[1] + x[1]**2.0 + 9.0*x[0] - 6*x[1] + 20\n",
    " \n",
    "#  определяем градиент функции\n",
    "def grad_func(x):\n",
    "    return np.array([2*x[0] - x[1] + 9, -x[0] + 2* x[1] - 6])\n",
    " \n",
    "# определяем начальную точку\n",
    "x_0 = [-400, -400]\n",
    "# реализуем алгоритм L-BFGS-B\n",
    "result = minimize(func, x_0, method='L-BFGS-B', jac=grad_func)\n",
    "# получаем результат\n",
    "print('Статус оптимизации %s' % result['message'])\n",
    "print('Количество оценок: %d' % result['nfev'])\n",
    "solution = result['x']\n",
    "evaluation = func(solution)\n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдите минимум функции x**2 - 3 *x + 45 с помощью квазиньютоновского метода BFGS.\n",
    "\n",
    "В качестве стартовой точки возьмите x = 10.\n",
    "\n",
    "В качестве ответа введите минимальное значение функции в достигнутой точке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статус оптимизации CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
      "Количество оценок: 3\n",
      "Решение: f([1.5]) = 42.75000\n"
     ]
    }
   ],
   "source": [
    "# определяем нашу функцию\n",
    "def func(x):\n",
    "    return x[0]**2.0 - 3*x[0] + 45\n",
    " \n",
    "#  определяем градиент функции\n",
    "def grad_func(x):\n",
    "    return np.array([2*x[0] - 3])\n",
    " \n",
    "# определяем начальную точку\n",
    "x_0 = 10\n",
    "# реализуем алгоритм L-BFGS-B\n",
    "result = minimize(func, x_0, method='L-BFGS-B', jac=grad_func)\n",
    "# получаем результат\n",
    "print('Статус оптимизации %s' % result['message'])\n",
    "print('Количество оценок: %d' % result['nfev'])\n",
    "solution = result['x']\n",
    "evaluation = func(solution)\n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решите предыдущую задачу, применяя модификацию L-BFGS-B.\n",
    "\n",
    "В каком случае получилось меньше итераций?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статус оптимизации Optimization terminated successfully.\n",
      "Количество оценок: 5\n",
      "Решение: f([1.5]) = 42.75000\n"
     ]
    }
   ],
   "source": [
    "# определяем нашу функцию\n",
    "def func(x):\n",
    "    return x[0]**2.0 - 3*x[0] + 45\n",
    " \n",
    "#  определяем градиент функции\n",
    "def grad_func(x):\n",
    "    return np.array([2*x[0] - 3])\n",
    " \n",
    "# определяем начальную точку\n",
    "x_0 = 10\n",
    "# реализуем алгоритм L-BFGS-B\n",
    "result = minimize(func, x_0, method='BFGS', jac=grad_func)\n",
    "# получаем результат\n",
    "print('Статус оптимизации %s' % result['message'])\n",
    "print('Количество оценок: %d' % result['nfev'])\n",
    "solution = result['x']\n",
    "evaluation = func(solution)\n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдите минимум функции x** 4 + 6*y**2 + 10, взяв за стартовую точку (100,100).\n",
    "\n",
    "Какой алгоритм сошелся быстрее?\n",
    "\n",
    "Каково минимальное значение функции в найденной точке минимума?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статус оптимизации CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "Количество оценок: 40\n",
      "Решение: f([-9.52718297e-03 -2.32170510e-06]) = 10.00000\n"
     ]
    }
   ],
   "source": [
    "# определяем нашу функцию\n",
    "def func(x):\n",
    "    return x[0]**4.0 + 6.0 * (x[1]**2.0) + 10\n",
    " \n",
    "#  определяем градиент функции\n",
    "def grad_func(x):\n",
    "    return np.array([4*(x[0]**3),12*x[1]])\n",
    " \n",
    "# определяем начальную точку\n",
    "x_0 = [100, 100]\n",
    "# реализуем алгоритм L-BFGS-B\n",
    "result = minimize(func, x_0, method='L-BFGS-B', jac=grad_func)\n",
    "# получаем результат\n",
    "print('Статус оптимизации %s' % result['message'])\n",
    "print('Количество оценок: %d' % result['nfev'])\n",
    "solution = result['x']\n",
    "evaluation = func(solution)\n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статус оптимизации Optimization terminated successfully.\n",
      "Количество оценок: 37\n",
      "Решение: f([1.31617159e-02 6.65344582e-14]) = 10.00000\n"
     ]
    }
   ],
   "source": [
    "# определяем нашу функцию\n",
    "def func(x):\n",
    "    return x[0]**4.0 + 6.0 * (x[1]**2.0) + 10\n",
    " \n",
    "#  определяем градиент функции\n",
    "def grad_func(x):\n",
    "    return np.array([4*(x[0]**3),12*x[1]])\n",
    " \n",
    "# определяем начальную точку\n",
    "x_0 = [100, 100]\n",
    "# реализуем алгоритм L-BFGS-B\n",
    "result = minimize(func, x_0, method='BFGS', jac=grad_func)\n",
    "# получаем результат\n",
    "print('Статус оптимизации %s' % result['message'])\n",
    "print('Количество оценок: %d' % result['nfev'])\n",
    "solution = result['x']\n",
    "evaluation = func(solution)\n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c68eacbc5a3a550d9b1f68bf11bf31e2b39ed4b9985227d4d8c7ee1d286013f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
