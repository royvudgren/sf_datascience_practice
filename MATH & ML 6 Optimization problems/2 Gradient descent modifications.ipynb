{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Градиентный спуск: применение и модификации**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*✍ В предыдущем модуле мы познакомились с алгоритмом градиентного спуска, а также с его очень популярной вариацией — градиентным спуском с momentum. Но на самом деле у градиентного спуска очень много модификаций, и, поскольку это действительно самый известный алгоритм, он является основой множества методов оптимизации. В этом юните мы рассмотрим и сравним три вариации градиентного спуска, которые используются наиболее часто.*\n",
    "\n",
    "Чтобы лучше понимать, какую роль играют методы оптимизации в DS и почему их так много, важно понимать основные **принципы работы нейронных сетей**. Также в современных статьях и обзорах разных методов оптимизации они практически всегда рассматриваются в контексте применения в нейронных сетях.\n",
    "\n",
    "Процесс обучения человеческого мозга очень сложен, и наука пока не может дать достаточно подробный ответ на вопрос о том, как мы получаем и усваиваем знания, как принимаем решения. Однако той информации, которую уже удалось получить, оказалось достаточно, чтобы по аналогии создать модели искусственных нейронных сетей.\n",
    "\n",
    "Люди учатся через пробы и ошибки, через процесс **синаптической пластичности**(это понятие, которое используется для описания того, как формируются и укрепляются нейронные связи после получения новой информации). Точно так же, как связи в мозге укрепляются и формируются по мере того, как мы переживаем новые события, мы обучаем искусственные нейронные сети, вычисляя ошибки нейросетевых предсказаний и усиливая или ослабляя внутренние связи между нейронами на основе этих ошибок.\n",
    "\n",
    "Для лучшего понимания удобно воспринимать нейронную сеть как **функцию**, которая принимает входные данные для получения итогового прогноза. Переменными этой функции являются параметры, или **веса нейрона**.\n",
    "\n",
    "Следовательно, ключевым моментом для решения задачи, которую мы ставим для нейронной сети, будет **корректировка значений весов** таким образом, чтобы они аппроксимировали или наилучшим образом представляли набор данных.\n",
    "\n",
    "На изображении ниже показана простая нейронная сеть, которая получает входные данные (X1,X2,X3,Xn). Эти входные данные передаются нейронам в слое, содержащем веса (W1,W2,W3,Wn). Входные данные и веса подвергаются операции умножения, и результат суммируется с помощью специальной функции, а **функция активации**(сигмоида) регулирует конечный результат модели.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/604e30ced0fcce4fc8de6700e3712df1/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/MATHML_md6_2_1.png)\n",
    "\n",
    "Для того чтобы оценить, насколько эффективно работает наша нейронная сеть, необходим показатель оценки разницы между предсказанием нейронной сети и фактическим значением целевой функции, позволяющий корректировать параметры сети так, чтобы разница между прогнозом и реальностью была как можно меньше. В Data Science эту разницу часто называют **функцией стоимости**.\n",
    "\n",
    "Ниже представлена модель работы простейшей нейронной сети:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/a8edf804d82a38c7dca11e9e3186d3d6/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/MATHML_md6_2_2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом изображении мы можем видеть модель простой нейронной сети из плотно связанных нейронов, которая классифицирует рукописные представления цифр 0, 1, 2, 3. Каждый нейрон в выходном слое соответствует цифре. Чем выше активация соединения с нейроном, тем выше вероятность, выдаваемая нейроном. Вероятность соответствует вероятности того, что цифра, переданная вперёд по сети, связана с активированным нейроном.\n",
    "\n",
    "Когда цифра 3 передаётся через сеть, мы ожидаем, что соединения (представленные на диаграмме стрелками), ответственные за классификацию этой цифры, будут иметь более высокую активацию, что приводит к более высокой вероятности для выходного нейрона, связанного с цифрой 3.\n",
    "\n",
    "Если объяснить происходящее более простыми словами, то механизм работы примерно такой:\n",
    "\n",
    "1. На первом слое мы выделяем из цифры какие-то первичные признаки (округлости, палочки).\n",
    "2. На втором слое мы выделяем уже какие-то паттерны (фрагменты цифры).\n",
    "3. На третьем слое паттерны складываются в целую цифру и мы можем предсказать результат.\n",
    "\n",
    "Каждый раз элементы, более похожие на элементы цифры 3, дают более высокую активацию.\n",
    "\n",
    "За активацию нейрона отвечает несколько компонентов, и мы должны менять их в процессе обучения нашей нейронной сети, повышая качество её предсказания.\n",
    "\n",
    "Допустим, мы используем уже хорошо известную вам среднеквадратичную ошибку как меру для оценки качества модели. В этом случае мы каждый раз вычисляем её, получаем результат и отправляем его обратно для коррекции весов в сети. Здесь как раз и появляется понятие **обратного распространения ошибки**.\n",
    "***\n",
    "* **Обратное распространение (backpropagation)** — это механизм, с помощью которого компоненты, влияющие на итоговый результат, итеративно корректируются для уменьшения функции стоимости.\n",
    "***\n",
    "Важнейшим математическим процессом, связанным с обратным распространением, является вычисление производных. Операции обратного распространения вычисляют частную производную функции стоимости по отношению к весам и активациям предыдущего слоя, чтобы определить, какие значения влияют на градиент функции стоимости.\n",
    "\n",
    "→ В процессе минимизации  функции ошибки мы постоянно вычисляем значение градиентов и таким образом приходим к локальному минимуму. На каждом этапе обучения нейронной сети её веса пересчитываются с помощью найденного градиента, причём скорость обучения (или, мы уже называли её ранее, темп обучения или шаг градиентного спуска) определяет коэффициент, с которым вносятся изменения в значения весов. Это повторяется на каждом шаге обучения нейронной сети. Нашей целью является постоянное приближение к локальному минимуму.\n",
    "\n",
    "Принцип обратного распространения ошибки заключается в том, что сначала мы устанавливаем какие-то случайные веса (параметры) для модели, находим итоговую ошибку и движемся по сети обратно, корректируя веса (для этого вычисляем частные производные, т. е. градиент).\n",
    "\n",
    "Данный процесс выглядит следующим образом:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/b19046193292c57cf06f2bc0532ddc9d/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/MATHML_md6_2_3.gif)\n",
    "\n",
    "Распространение ошибок и использование частных производных для корректировки весов происходит до тех пор, пока не будут скорректированы все параметры в сети вплоть до первого слоя.\n",
    "\n",
    "Как вы можете видеть, даже в самом простом примере нейронной сети много переменных и действий, которые с ними происходят. Из-за этого ландшафт функции потерь для нейронных сетей становится очень сложным. К примеру, ландшафт для нейронной сети с 56 слоями может выглядеть так:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/dcd9e0bd78da224572e761ce0358ead3/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/MATHML_md6_2_4.png)\n",
    "\n",
    "В предыдущем модуле мы без проблем решили задачу с помощью классического градиентного спуска, и результат совпал с итогом, полученным с помощью реализации алгоритма в стандартной библиотеке Python. Но функция потерь там была совершенно другой. Когда дело доходит до искусственных нейронных сетей, алгоритмы оптимизации сталкиваются с множеством проблем."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Основные проблемы при реализации градиентного спуска:**\n",
    "\n",
    "* Классический градиентный спуск склонен застревать в точках локального минимума и даже в седловых точках, словом — везде, где градиент равен нулю. Это мешает найти глобальный минимум.\n",
    "* Обычно у оптимизируемой функции очень сложный ландшафт: где-то она совсем пологая, где-то более крутой обрыв. В таких ситуациях градиентный спуск показывает не лучшие результаты. Так происходит потому, что в алгоритме градиентного спуска фиксированный шаг, а нам в идеале хотелось бы его изменять в зависимости от формы функции прямо в процессе обучения.\n",
    "* Много проблем возникает из-за темпа обучения: при низком алгоритм сходится невероятно медленно, при быстром — «пролетает» мимо минимумов.\n",
    "* При обучении градиентного спуска координаты в некоторых измерениях могут редко изменяться, что приводит к плохой обобщающей способности алгоритма. Можно попытаться придать каждому признаку бόльшую важность, но в таком случае есть серьёзный риск переобучить модель.\n",
    "\n",
    "Тем не менее, нельзя отрицать, что градиентный спуск — невероятно эффективный и популярный алгоритм. Допустим, он прекрасно подходит для минимизации среднеквадратичной ошибки в случае решения задачи регрессии. Однако в силу его несовершенств, которые очень явно проявляются в сложных моделях (например, в нейронных сетях), были созданы некоторые его модификации, которые позволяют решать задачи с большей результативностью.\n",
    "\n",
    "Обычно выделяют **три основных вариации градиентного спуска**:\n",
    "\n",
    "1. **Batch Gradient Descent**;\n",
    "2. **Stochastic Gradient Descent**;\n",
    "3. **Mini-batch Gradient Descent**.\n",
    "\n",
    "Далее мы рассмотрим все эти модификации и обсудим их различия. Важно понимать, что, несмотря на то что про их применение часто говорят именно в контексте нейронных сетей, они прекрасно подходят и для использования с обычными методами машинного обучения. К примеру, для стандартной линейной регрессии также подходит и обычный градиентный спуск."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## **BATCH GRADIENT DESCENT**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первая вариация — это Batch Gradient Descent. По-русски её называют пакетным градиентным спуском (он использует всю выборку (весь пакет) на каждом шаге, для того чтобы получить результат), или ванильным градиентным спуском (хотя англоязычную вариацию Vanilla Gradient Descent чаще не переводят). По сути, это и есть классический градиентный спуск, который мы с вами рассматривали в предыдущем модуле.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/ad02f9aeba9cab2f06cf437e01c411f8/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/MATHML_md6_2_5.png)\n",
    "\n",
    "Часто в различных источниках шаг Batch Gradient Descent записывается в следующих обозначениях:\n",
    "\n",
    "![](data/1.PNG)\n",
    "\n",
    "Примечание. На самом деле совершенно не важно, какими буквами выражать те или иные объекты, но мы будем использовать в этом модуле обозначения, которые часто встречаются в научных статьях и различной литературе.\n",
    "\n",
    "Здесь ***θ*** — вектор с параметрами функции, ***η*** — шаг градиента, ![](data/2.PNG) — градиент функции, найденный по её параметрам.\n",
    "\n",
    "Таким образом, на каждом шаге градиентный спуск находит направление наискорейшего убывания функции и движется чётко по нему. Поэтому для несложных функций он достаточно быстро сходится. Его обучение можно изобразить следующим образом:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/a4938996e0fb2e0f0a00193ad27c631d/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/MATHML_md6_2_6.png)\n",
    "\n",
    "Такой градиентный спуск достаточно хорошо работает, если мы рассматриваем выпуклые или относительно гладкие функции ошибки. Такие, к примеру, мы можем наблюдать у линейной или логистической регрессии. Мы обсуждали, что градиентный спуск не умеет находить глобальный минимум среди прочих, но, например, для выпуклой функции он может это сделать. Поэтому с выпуклыми функциями (допустим, со среднеквадратичной ошибкой для линейной регрессии) нам удобнее всего использовать именно его.\n",
    "\n",
    "Но всё усложняется, когда мы хотим применить его при обучении нейронных сетей. Как уже говорилось, у таких моделей функция потерь имеет много локальных экстремумов, в каждом из которых градиентный спуск может застрять. Когда данных очень много, а оптимизируемая функция очень сложная, данный алгоритм применять затруднительно, так как поиск градиента по всем наблюдениям делает задачу очень затратной в плане вычислительных ресурсов."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## **STOCHASTIC GRADIENT DESCENT**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представим, что мы реализуем градиентный спуск для набора данных объёмом 10 000 наблюдений и у нас десять переменных. Среднеквадратичную ошибку считаем по всем точкам, то есть для 10 000 наблюдений. Производную необходимо посчитать по каждому параметру, поэтому фактически за каждую итерацию мы будем выполнять не менее 100 000 вычислений. И если, допустим, у нас 1000 итераций, то нам нужно 100000*1000=100000000 вычислений. Это довольно много, поэтому градиентный спуск на сложных моделях и при использовании больших наборов данных работает крайне долго.\n",
    "\n",
    "Чтобы преодолеть эту проблему, придумали **стохастический градиентный спуск**. Слово «стохастический» можно воспринимать как синоним слова «случайный». Где же при использовании градиентного спуска может возникнуть случайность? При выборе данных. При реализации стохастического спуска вычисляются градиенты не для всей выборки, а только для случайно выбранной единственной точки.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/34c59eb72fab899262cf5514d4981e6a/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/MATHML_md6_2_7.png)\n",
    "\n",
    "Это значительно сокращает вычислительные затраты.\n",
    "\n",
    "В виде формулы это можно записать следующим образом:\n",
    "\n",
    "![](data/3.PNG)\n",
    "\n",
    "На визуальном представлении ниже можно увидеть, что стохастический спуск создаёт много колебаний при сходимости. Это происходит как раз за счёт того, что берётся не вся выборка, а только один объект, и между объектами может быть достаточно большая разница. Чем меньше выборка, тем меньше стабильности при реализации.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/aa52e836e3a7b359b496351d8a1c4b62/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/MATHML_md6_2_8.png)\n",
    "\n",
    "Стохастический градиентный спуск очень часто используется в нейронных сетях и сокращает время машинных вычислений, одновременно повышая сложность и производительность крупномасштабных задач."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## **MINI-BATCH GRADIENT DESCENT**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Третья вариация градиентного спуска — **Mini-batch Gradient Descent**. Также можно называть его **мини-пакетным градиентным спуском**. По сути, эта модификация сочетает в себе лучшее от классической реализации и стохастического варианта. На данный момент это наиболее популярная реализация градиентного спуска, которая используется в глубоком обучении (т. е. в обучении нейронных сетей).\n",
    "\n",
    "В ходе обучения модели с помощью мини-пакетного градиентного спуска обучающая выборка разбивается на **пакеты (батчи)**, для которых рассчитывается ошибка модели и пересчитываются веса.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/29566a5834ca122fa8aa342003c67441/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/MATHML_md6_2_9.png)\n",
    "\n",
    "То есть, с одной стороны, мы используем все преимущества обычного градиентного спуска, а с другой — уменьшаем сложность вычислений и повышаем их скорость по аналогии со стохастическим спуском. Кроме того, алгоритм работает ещё быстрее за счёт возможности применения векторизованных вычислений.\n",
    "\n",
    "Формализовать это можно следующим образом:\n",
    "\n",
    "![](data/4.PNG)\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Резюмируем сравнение трёх вариаций градиентного спуска:**\n",
    "\n",
    "![](data/5.PNG)\n",
    "\n",
    "✍ Итак, мы рассмотрели алгоритмы градиентного спуска и сравнили их параметры. Теперь, если вам необходимо будет выбрать метод градиентного спуска, вы сможете понять, какой из них подходит лучше всего."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте потренируемся применять стохастический градиентный спуск для решения задачи линейной регрессии. Мы уже рассмотрели его реализацию «с нуля», однако для решения практических задач можно использовать готовые библиотеки.\n",
    "\n",
    "Загрузите стандартный датасет об алмазах из библиотеки Seaborn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.0026826957952797246, 'eta0': 0.001, 'l1_ratio': 0.0, 'learning_rate': 'constant', 'loss': 'epsilon_insensitive', 'max_iter': 21.544346900318832, 'penalty': 'elasticnet'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.043"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = sns.load_dataset('diamonds')\n",
    "\n",
    "# Удалите часть признаков:\n",
    "df.drop(['depth', 'table', 'x', 'y', 'z'], axis=1, inplace=True)\n",
    "\n",
    "#Закодируйте категориальные признаки:\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "#Логарифмируйте признаки:\n",
    "df['carat'] = np.log(1+df['carat'])\n",
    "df['price'] = np.log(1+df['price'])\n",
    "\n",
    "#Определите целевую переменную и предикторы:\n",
    "X_cols = [col for col in df.columns if col!='price'] \n",
    "X = df[X_cols]\n",
    "y = df['price']\n",
    "\n",
    "# Разделите выборку на обучающую и тестовую (объём тестовой возьмите равным 0.33).\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Теперь реализуйте алгоритм линейной регрессии со стохастическим градиентным спуском\n",
    "# (функция SGDRegressor). Отберите с помощью gridsearch() оптимальные параметры по следующей сетке:\n",
    "parameters = {\n",
    "    'learning_rate': ['constant'],\n",
    "    'eta0': np.logspace(-4,-1,4),\n",
    "    'max_iter': np.logspace(0,3,10),\n",
    "    'loss': ['squared_error', 'epsilon_insensitive'],\n",
    "    'penalty': ['elasticnet'],\n",
    "    'alpha': np.logspace(-3,3,15),\n",
    "    'l1_ratio': np.linspace(0,1,11)\n",
    "}\n",
    "# Обучите регрессию с оптимальными параметрами. В качестве ответа введите получившееся значение MSE,\n",
    "# предварительно округлив его до третьего знака после точки-разделителя.\n",
    "sgd = SGDRegressor(random_state=42)\n",
    "sgd_cv = GridSearchCV(estimator=sgd, param_grid=parameters, n_jobs=-1)\n",
    "sgd_cv.fit(X_train, y_train)\n",
    "\n",
    "print(sgd_cv.best_params_)\n",
    "\n",
    "sgd = SGDRegressor(**sgd_cv.best_params_, random_state = 42)\n",
    "\n",
    "sgd.fit(X_train, y_train)\n",
    "sgd.score(X_train, y_train) # r2\n",
    "ls = sgd.predict(X_test)\n",
    "\n",
    "round(mean_squared_error(y_test, ls), 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## ***БОНУС: АЛГОРИТМЫ, ОСНОВАННЫЕ НА ГРАДИЕНТНОМ СПУСКЕ**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный спуск настолько популярен и хорошо применим для решения различных задач, что послужил основой множества дополнительных методов. Поговорим про некоторые из них.\n",
    "\n",
    "Иногда в данных присутствуют очень редко встречающиеся входные параметры.\n",
    "\n",
    "Например, если мы классифицируем письма на «Спам» и «Не спам», таким параметром может быть очень специфическое слово, которое встречается в спаме намного реже других слов-индикаторов. Или, если мы говорим о распознавании изображений, это может быть какая-то очень редкая характеристика объекта.\n",
    "\n",
    "В таком случае нам хотелось бы иметь для каждого параметра свою скорость обучения: чтобы для часто встречающихся она была низкой (для более точной настройки), а для совсем редких — высокой (это повысит скорость сходимости). То есть нам очень важно уметь обновлять параметры модели, учитывая то, насколько типичные и значимые признаки они кодируют.\n",
    "\n",
    "Решение этой задачи предложено в рамках алгоритма **AdaGrad** (его название обозначает, что это адаптированный градиентный спуск). В нём обновления происходят по следующему принципу:\n",
    "\n",
    "![](data/6.PNG)\n",
    "\n",
    "Здесь мы храним сумму квадратов градиентов для каждого параметра. Таким образом, параметры, которые сильно обновляются каждый раз, начинают обновляться слабее. Скорость обучения в таком алгоритме будет постоянно затухать. Мы будем начинать с больших шагов, и с приближением к точке минимума шаги будут уменьшаться — это улучшит скорость сходимости.\n",
    "\n",
    "Данный алгоритм достаточно популярен и работает лучше стохастического градиентного спуска. Его использует и компания Google в своих алгоритмах классификации изображений.\n",
    "\n",
    "Однако снижение скорости обучения в AdaGrad иногда происходит слишком радикально, и она практически обнуляется. Чтобы решить эту проблему, были созданы алгоритмы **RMSProp, AdaDelta, Adam** и некоторые другие.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c68eacbc5a3a550d9b1f68bf11bf31e2b39ed4b9985227d4d8c7ee1d286013f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
