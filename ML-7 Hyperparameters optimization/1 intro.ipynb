{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Введение**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, как мы уже неоднократно упоминали ранее, в машинном обучении есть два типа параметров.\n",
    "\n",
    "**Внутренние (параметры модели)**  \n",
    "Подбираются во время обучения и определяют, как использовать входные данные для получения необходимого результата.  \n",
    "Например, это веса (коэффициенты уравнения) в линейной/логистической регрессии.  \n",
    "\n",
    "**Внешние (параметры алгоритма)**  \n",
    "Их принято называть **гиперпараметрами**. Внешние параметры могут быть произвольно установлены перед началом обучения и контролируют внутреннюю работу обучающего алгоритма.  \n",
    "Например, это параметр регуляризации в линейной/логистической регрессии.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Гиперпараметры** отвечают за сложность взаимосвязи между входными признаками и целевой переменной, поэтому сильно влияют на модель и качество прогнозирования.\n",
    "\n",
    "Продемонстрируем это на примере задачи регрессии с помощью двух графиков работы алгоритма случайного леса, построенного на основе 5, 100 деревьев (n_estimators = [5, 100]):\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/bae7f1720a67839d129d3bf7e82f3450/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst-3-ml-7-1.png)\n",
    "\n",
    "Видим, что при 100 деревьях модель находит более сложную закономерность в данных и точность соответственно будет выше, чем при 5.\n",
    "\n",
    "Каждый алгоритм МО имеет набор гиперпараметров, которые определяют, как именно он строит модель на обучающей выборке. Например, в модуле ML-2 для повышения эффективности модели мы уже рассматривали подбор параметра регуляризации  для алгоритма линейной регрессии Ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаем список из 20 возможных значений от 0.001 до 10\n",
    "alpha_list = np.linspace(0.01, 10, 20)\n",
    "#Создаем пустые списки, в которые будем добавлять результаты \n",
    "train_scores = []\n",
    "test_scores = []\n",
    "for alpha in alpha_list:\n",
    "    #Создаем объект класса линейная регрессия с L2-регуляризацией\n",
    "    ridge_lr_poly = linear_model.Ridge(alpha=alpha, max_iter=10000)\n",
    "    #Обучаем модель предсказывать логарифм целевого признака\n",
    "    ridge_lr_poly.fit(X_train_scaled_poly, y_train_log)\n",
    "    #Делаем предсказание для каждой из выборок\n",
    "    #Если обучили на логарифме, то от результата необходимо взять обратную функцию - экспоненту\n",
    "    y_train_predict_poly = np.exp(ridge_lr_poly.predict(X_train_scaled_poly))\n",
    "    y_test_predict_poly = np.exp(ridge_lr_poly.predict(X_test_scaled_poly))\n",
    "    #Рассчитываем метрику для двух выборок и добавляем их в списки\n",
    "    train_scores.append(metrics.mean_absolute_error(y_train, y_train_predict_poly))\n",
    "    test_scores.append(metrics.mean_absolute_error(y_test, y_test_predict_poly))\n",
    " \n",
    "#Визуализируем изменение R^2 в зависимости от alpha\n",
    "fig, ax = plt.subplots(figsize=(12, 4)) #фигура + координатная плоскость\n",
    "ax.plot(alpha_list, train_scores, label='Train') #линейный график для тренировочной выборки\n",
    "ax.plot(alpha_list, test_scores, label='Test') #линейный график для тестовой выборки\n",
    "ax.set_xlabel('Alpha') #название оси абсцисс\n",
    "ax.set_ylabel('MAE') #название оси ординат\n",
    "ax.set_xticks(alpha_list) #метки по оси абцисс\n",
    "ax.xaxis.set_tick_params(rotation=45) #поворот меток на оси абсцисс\n",
    "ax.legend(); #отображение легенды"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://lms.skillfactory.ru/assets/courseware/v1/47a7e0d5d24abca72171988571c18d13/asset-v1:SkillFactory+DSPR-2.0+14JULY2021+type@asset+block/dst-3-ml-7-2.png)\n",
    "\n",
    "Наилучшее значение метрики соответствует α = 0.01 (кстати, можно попробовать перебрать значения α < 0.01).\n",
    "\n",
    "В данном случае мы просто воспользовались циклом for и перебрали некоторые заданные значения alpha, хотя, по всей видимости, не самые оптимальные. Поэтому подобранные эмпирическим путём значения гиперпараметров с большей вероятностью дадут низкую прогностическую эффективность.\n",
    "\n",
    "Также рассмотренный метод визуализации зависимости метрики от гиперпараметра позволяет выбрать только один внешний параметр, в данном случае — alpha.\n",
    "\n",
    "А что делать, если у нас не один параметр, а несколько? \n",
    "***\n",
    "Например, вспомним основные внешние параметры **DecisionTreeClassifier**:\n",
    "\n",
    "* **criterion** — критерий информативности. Может быть равен 'gini' — критерий Джини — и 'entropy' — энтропия Шеннона.\n",
    "* **max_depth** — максимальная глубина дерева. По умолчанию None, глубина дерева не ограничена.\n",
    "* **max_features** — максимальное число признаков, по которым ищется лучшее разбиение в дереве. По умолчанию None, то есть обучение производится на всех признаках.\n",
    "* **min_samples_leaf** — минимальное число объектов в листе. По умолчанию — 1.\n",
    "\n",
    "Мы, конечно, можем сделать кучу вложенных циклов. Однако, поскольку поиск оптимальных значений гиперпараметров является общераспространенной задачей МО, библиотека scikit-learn и другие предлагают методы, позволяющие её решить.\n",
    "\n",
    "Тщательный подбор гиперпараметров гарантирует, что модель покажет максимально возможную точность на обучающих данных, но это **совершенно не означает хороший результат на тестовых или новых данных**.\n",
    "\n",
    "Поиск оптимальных значений гиперпараметров модели является сложной задачей, обязательной почти для всех моделей и наборов данных. Однако важно понимать смысл гиперпараметров перед их подбором.\n",
    "\n",
    "**ЦЕЛИ МОДУЛЯ**\n",
    "\n",
    "* Узнать, какие есть базовые способы оптимизации гиперпараметров (GridSearchCV, RandomSearchCV).\n",
    "* Узнать, какие есть продвинутые способы оптимизации (Hyperopt, Optuna).\n",
    "* Научиться их настраивать и обучать модели с их использованием — так, чтобы улучшать итоговую метрику.  \n",
    "* Провести сравнение и понять преимущества и недостатки каждого из методов."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c68eacbc5a3a550d9b1f68bf11bf31e2b39ed4b9985227d4d8c7ee1d286013f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
